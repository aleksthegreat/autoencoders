{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wopr/.local/lib/python3.6/site-packages/tensorflow/python/ops/distributions/distribution.py:265: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/wopr/.local/lib/python3.6/site-packages/tensorflow/python/ops/distributions/bernoulli.py:169: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from scipy.stats import norm, rankdata\n",
    "\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add,PReLU, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Reshape, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import tensorflow as tf\n",
    "import horovod.keras as hvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horovod: initialize Horovod.\n",
    "hvd.init()\n",
    "\n",
    "# Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 78.01 MB\n",
      "Decreased by 74.7%\n",
      "Memory usage after optimization is: 77.82 MB\n",
      "Decreased by 74.6%\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(pd.read_csv('../input/train.csv'))\n",
    "test = reduce_mem_usage(pd.read_csv('../input/test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f for f in train if f not in ['ID_code','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.concat([train, test],axis=0,sort=False)\n",
    "df = df_original[features]\n",
    "target = df_original['target'].values\n",
    "id = df_original['ID_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for feature in features:\n",
    "#    df['mean_'+feature] = (train[feature].mean()-train[feature])\n",
    "#    df['z_'+feature] = (train[feature] - train[feature].mean())/train[feature].std(ddof=0)\n",
    "#    df['sq_'+feature] = (train[feature])**2\n",
    "#    df['sqrt_'+feature] = np.abs(train[feature])**(1/2)\n",
    "#    df['cp_'+feature] = pd.DataFrame(rankdata(train[feature]))\n",
    "#    df['cnp_'+feature] = pd.DataFrame((norm.cdf(train[feature])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wopr/.local/lib/python3.6/site-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "for df in [df]:\n",
    "#####Handling Missing Values#####     \n",
    "    for i in range(len(df.columns)):\n",
    "        df.iloc[:,i] = (df.iloc[:,i]).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wopr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 155.64 MB\n",
      "Decreased by 0.0%\n"
     ]
    }
   ],
   "source": [
    "df.isnull().values.any()\n",
    "df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wopr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import erfinv\n",
    "trafo_columns = [c for c in df.columns if len(df[c].unique()) != 2]\n",
    "for col in trafo_columns:\n",
    "    values = sorted(set(df[col]))\n",
    "    # Because erfinv(1) is inf, we shrink the range into (-0.9, 0.9)\n",
    "    f = pd.Series(np.linspace(-0.9, 0.9, len(values)), index=values)\n",
    "    f = np.sqrt(2) * erfinv(f)\n",
    "    f -= f.mean()\n",
    "    df[col] = df[col].map(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=200)\n",
    "pca.fit(df[trafo_columns])\n",
    "df = pca.transform(df[trafo_columns])\n",
    "df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wopr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/wopr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 156.40 MB\n",
      "Decreased by 74.6%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.276123</td>\n",
       "      <td>-1.248047</td>\n",
       "      <td>0.492676</td>\n",
       "      <td>0.178345</td>\n",
       "      <td>0.301514</td>\n",
       "      <td>-1.103516</td>\n",
       "      <td>0.089050</td>\n",
       "      <td>0.470459</td>\n",
       "      <td>-1.241211</td>\n",
       "      <td>-0.489014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264893</td>\n",
       "      <td>1.095703</td>\n",
       "      <td>0.690430</td>\n",
       "      <td>0.287354</td>\n",
       "      <td>-1.189453</td>\n",
       "      <td>1.255859</td>\n",
       "      <td>0.127563</td>\n",
       "      <td>-0.142212</td>\n",
       "      <td>-0.439697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.544922</td>\n",
       "      <td>-1.052734</td>\n",
       "      <td>0.788086</td>\n",
       "      <td>0.232422</td>\n",
       "      <td>0.491943</td>\n",
       "      <td>1.179688</td>\n",
       "      <td>0.271484</td>\n",
       "      <td>0.289795</td>\n",
       "      <td>1.054688</td>\n",
       "      <td>0.565918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720703</td>\n",
       "      <td>1.007812</td>\n",
       "      <td>1.376953</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>1.229492</td>\n",
       "      <td>1.270508</td>\n",
       "      <td>0.197388</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.682129</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.245361</td>\n",
       "      <td>-0.890137</td>\n",
       "      <td>0.516602</td>\n",
       "      <td>0.739258</td>\n",
       "      <td>0.126343</td>\n",
       "      <td>-1.097656</td>\n",
       "      <td>0.821289</td>\n",
       "      <td>0.021896</td>\n",
       "      <td>-1.241211</td>\n",
       "      <td>-0.388916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892090</td>\n",
       "      <td>0.833008</td>\n",
       "      <td>0.689453</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>1.481445</td>\n",
       "      <td>-1.262695</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>0.174316</td>\n",
       "      <td>0.313965</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.497070</td>\n",
       "      <td>-0.824707</td>\n",
       "      <td>0.111084</td>\n",
       "      <td>0.583984</td>\n",
       "      <td>0.542480</td>\n",
       "      <td>-0.621582</td>\n",
       "      <td>0.354492</td>\n",
       "      <td>0.070740</td>\n",
       "      <td>-1.325195</td>\n",
       "      <td>0.606445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355225</td>\n",
       "      <td>0.544922</td>\n",
       "      <td>0.640137</td>\n",
       "      <td>0.805664</td>\n",
       "      <td>-0.897949</td>\n",
       "      <td>-0.942871</td>\n",
       "      <td>0.707520</td>\n",
       "      <td>0.571777</td>\n",
       "      <td>-0.994141</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.368164</td>\n",
       "      <td>-0.703613</td>\n",
       "      <td>0.632324</td>\n",
       "      <td>0.469238</td>\n",
       "      <td>0.472900</td>\n",
       "      <td>0.818848</td>\n",
       "      <td>0.391846</td>\n",
       "      <td>0.526855</td>\n",
       "      <td>1.381836</td>\n",
       "      <td>0.387207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871094</td>\n",
       "      <td>-0.407227</td>\n",
       "      <td>1.296875</td>\n",
       "      <td>-0.517090</td>\n",
       "      <td>-0.965820</td>\n",
       "      <td>0.975586</td>\n",
       "      <td>0.424805</td>\n",
       "      <td>0.574707</td>\n",
       "      <td>-0.988770</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var_0     var_1     var_2     var_3     var_4     var_5     var_6  \\\n",
       "0  0.276123 -1.248047  0.492676  0.178345  0.301514 -1.103516  0.089050   \n",
       "1  0.544922 -1.052734  0.788086  0.232422  0.491943  1.179688  0.271484   \n",
       "2  0.245361 -0.890137  0.516602  0.739258  0.126343 -1.097656  0.821289   \n",
       "3  0.497070 -0.824707  0.111084  0.583984  0.542480 -0.621582  0.354492   \n",
       "4  0.368164 -0.703613  0.632324  0.469238  0.472900  0.818848  0.391846   \n",
       "\n",
       "      var_7     var_8     var_9   ...     var_191   var_192   var_193  \\\n",
       "0  0.470459 -1.241211 -0.489014   ...    0.264893  1.095703  0.690430   \n",
       "1  0.289795  1.054688  0.565918   ...    0.720703  1.007812  1.376953   \n",
       "2  0.021896 -1.241211 -0.388916   ...    0.892090  0.833008  0.689453   \n",
       "3  0.070740 -1.325195  0.606445   ...    0.355225  0.544922  0.640137   \n",
       "4  0.526855  1.381836  0.387207   ...    0.871094 -0.407227  1.296875   \n",
       "\n",
       "    var_194   var_195   var_196   var_197   var_198   var_199  target  \n",
       "0  0.287354 -1.189453  1.255859  0.127563 -0.142212 -0.439697     0.0  \n",
       "1 -0.075195  1.229492  1.270508  0.197388  0.609375  0.682129     0.0  \n",
       "2  0.625488  1.481445 -1.262695  0.036865  0.174316  0.313965     0.0  \n",
       "3  0.805664 -0.897949 -0.942871  0.707520  0.571777 -0.994141     0.0  \n",
       "4 -0.517090 -0.965820  0.975586  0.424805  0.574707 -0.988770     0.0  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'] = df_original.target.values\n",
    "df = reduce_mem_usage(df)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = df[df['target'].notnull()]\n",
    "#target = train['target']\n",
    "#test = df[df['target'].isnull()]\n",
    "#trafo_columns = [c for c in train.columns if c not in ['target']]\n",
    "#train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafo_columns = [c for c in df.columns if c not in ['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1500)              301500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1500)              6000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1500)              6000      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               300200    \n",
      "=================================================================\n",
      "Total params: 5,116,700\n",
      "Trainable params: 5,110,700\n",
      "Non-trainable params: 6,000\n",
      "_________________________________________________________________\n",
      "Train on 300000 samples, validate on 100000 samples\n",
      "Epoch 1/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 8.1071 - acc: 0.0143 - val_loss: 3.4614 - val_acc: 0.0211\n",
      "Epoch 2/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 1.3853 - acc: 0.0499 - val_loss: 0.7321 - val_acc: 0.0439\n",
      "Epoch 3/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7869 - acc: 0.0835 - val_loss: 0.6552 - val_acc: 0.0568\n",
      "Epoch 4/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7843 - acc: 0.0888 - val_loss: 0.6417 - val_acc: 0.0668\n",
      "Epoch 5/100\n",
      "300000/300000 [==============================] - 19s 63us/step - loss: 0.7812 - acc: 0.0936 - val_loss: 0.6626 - val_acc: 0.0671\n",
      "Epoch 6/100\n",
      "300000/300000 [==============================] - 19s 64us/step - loss: 0.7809 - acc: 0.0984 - val_loss: 0.6606 - val_acc: 0.0758\n",
      "Epoch 7/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7788 - acc: 0.1014 - val_loss: 0.6414 - val_acc: 0.0738\n",
      "Epoch 8/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7779 - acc: 0.1051 - val_loss: 0.6107 - val_acc: 0.0858\n",
      "Epoch 9/100\n",
      "300000/300000 [==============================] - 19s 64us/step - loss: 0.7767 - acc: 0.1085 - val_loss: 0.6083 - val_acc: 0.0838\n",
      "Epoch 10/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7748 - acc: 0.1119 - val_loss: 0.6155 - val_acc: 0.0860\n",
      "Epoch 11/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7751 - acc: 0.1143 - val_loss: 0.5858 - val_acc: 0.0962\n",
      "Epoch 12/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7733 - acc: 0.1170 - val_loss: 0.5872 - val_acc: 0.0957\n",
      "Epoch 13/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7708 - acc: 0.1208 - val_loss: 0.5979 - val_acc: 0.0954\n",
      "Epoch 14/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7702 - acc: 0.1242 - val_loss: 0.5879 - val_acc: 0.0980\n",
      "Epoch 15/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7701 - acc: 0.1280 - val_loss: 0.6154 - val_acc: 0.0943\n",
      "Epoch 16/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7686 - acc: 0.1321 - val_loss: 0.5768 - val_acc: 0.0995\n",
      "Epoch 17/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7679 - acc: 0.1329 - val_loss: 0.5616 - val_acc: 0.0981\n",
      "Epoch 18/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7664 - acc: 0.1375 - val_loss: 0.6172 - val_acc: 0.1022\n",
      "Epoch 19/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7643 - acc: 0.1386 - val_loss: 0.6134 - val_acc: 0.1001\n",
      "Epoch 20/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7642 - acc: 0.1429 - val_loss: 0.5587 - val_acc: 0.1104\n",
      "Epoch 21/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7632 - acc: 0.1455 - val_loss: 0.5755 - val_acc: 0.1160\n",
      "Epoch 22/100\n",
      "300000/300000 [==============================] - 19s 64us/step - loss: 0.7626 - acc: 0.1487 - val_loss: 0.6118 - val_acc: 0.1168\n",
      "Epoch 23/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7619 - acc: 0.1508 - val_loss: 0.5926 - val_acc: 0.1200\n",
      "Epoch 24/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7618 - acc: 0.1530 - val_loss: 0.6015 - val_acc: 0.1176\n",
      "Epoch 25/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7606 - acc: 0.1564 - val_loss: 0.5891 - val_acc: 0.1244\n",
      "Epoch 26/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7599 - acc: 0.1575 - val_loss: 0.5627 - val_acc: 0.1263\n",
      "Epoch 27/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7602 - acc: 0.1603 - val_loss: 0.5784 - val_acc: 0.1225\n",
      "Epoch 28/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7589 - acc: 0.1633 - val_loss: 0.5801 - val_acc: 0.1342\n",
      "Epoch 29/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7583 - acc: 0.1648 - val_loss: 0.6122 - val_acc: 0.1214\n",
      "Epoch 30/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7587 - acc: 0.1666 - val_loss: 0.5649 - val_acc: 0.1376\n",
      "Epoch 31/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7584 - acc: 0.1687 - val_loss: 0.5627 - val_acc: 0.1296\n",
      "Epoch 32/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7573 - acc: 0.1690 - val_loss: 0.5733 - val_acc: 0.1487\n",
      "Epoch 33/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7571 - acc: 0.1723 - val_loss: 0.5586 - val_acc: 0.1495\n",
      "Epoch 34/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7574 - acc: 0.1733 - val_loss: 0.5808 - val_acc: 0.1410\n",
      "Epoch 35/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7555 - acc: 0.1733 - val_loss: 0.5762 - val_acc: 0.1414\n",
      "Epoch 36/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7561 - acc: 0.1762 - val_loss: 0.5936 - val_acc: 0.1428\n",
      "Epoch 37/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7546 - acc: 0.1769 - val_loss: 0.5931 - val_acc: 0.1483\n",
      "Epoch 38/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7557 - acc: 0.1791 - val_loss: 0.5884 - val_acc: 0.1414\n",
      "Epoch 39/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7548 - acc: 0.1797 - val_loss: 0.5434 - val_acc: 0.1535\n",
      "Epoch 40/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7550 - acc: 0.1818 - val_loss: 0.5572 - val_acc: 0.1510\n",
      "Epoch 41/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7543 - acc: 0.1830 - val_loss: 0.5420 - val_acc: 0.1505\n",
      "Epoch 42/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7544 - acc: 0.1842 - val_loss: 0.5940 - val_acc: 0.1557\n",
      "Epoch 43/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7547 - acc: 0.1862 - val_loss: 0.5940 - val_acc: 0.1503\n",
      "Epoch 44/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7533 - acc: 0.1883 - val_loss: 0.5930 - val_acc: 0.1555\n",
      "Epoch 45/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7547 - acc: 0.1891 - val_loss: 0.5649 - val_acc: 0.1466\n",
      "Epoch 46/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7529 - acc: 0.1892 - val_loss: 0.5753 - val_acc: 0.1680\n",
      "Epoch 47/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7525 - acc: 0.1909 - val_loss: 0.5563 - val_acc: 0.1676\n",
      "Epoch 48/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7524 - acc: 0.1930 - val_loss: 0.5701 - val_acc: 0.1578\n",
      "Epoch 49/100\n",
      "300000/300000 [==============================] - 19s 64us/step - loss: 0.7531 - acc: 0.1955 - val_loss: 0.5698 - val_acc: 0.1735\n",
      "Epoch 50/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7522 - acc: 0.1958 - val_loss: 0.5666 - val_acc: 0.1656\n",
      "Epoch 51/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7517 - acc: 0.1976 - val_loss: 0.5540 - val_acc: 0.1621\n",
      "Epoch 52/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7514 - acc: 0.1995 - val_loss: 0.5207 - val_acc: 0.1946\n",
      "Epoch 53/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7517 - acc: 0.2016 - val_loss: 0.5492 - val_acc: 0.1405\n",
      "Epoch 54/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7500 - acc: 0.2032 - val_loss: 0.5581 - val_acc: 0.1695\n",
      "Epoch 55/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7510 - acc: 0.2052 - val_loss: 0.5638 - val_acc: 0.1616\n",
      "Epoch 56/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7505 - acc: 0.2059 - val_loss: 0.5586 - val_acc: 0.1794\n",
      "Epoch 57/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7505 - acc: 0.2079 - val_loss: 0.5667 - val_acc: 0.1597\n",
      "Epoch 58/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7509 - acc: 0.2098 - val_loss: 0.5729 - val_acc: 0.1843\n",
      "Epoch 59/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7508 - acc: 0.2126 - val_loss: 0.5579 - val_acc: 0.1850\n",
      "Epoch 60/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7502 - acc: 0.2125 - val_loss: 0.5495 - val_acc: 0.1814\n",
      "Epoch 61/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7501 - acc: 0.2143 - val_loss: 0.5659 - val_acc: 0.1707\n",
      "Epoch 62/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7481 - acc: 0.2155 - val_loss: 0.5336 - val_acc: 0.1592\n",
      "Epoch 00062: early stopping\n",
      "100000/100000 [==============================] - 5s 47us/step\n",
      "400000/400000 [==============================] - 18s 46us/step\n",
      "fold 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1500)              301500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1500)              6000      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1500)              6000      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 200)               300200    \n",
      "=================================================================\n",
      "Total params: 5,116,700\n",
      "Trainable params: 5,110,700\n",
      "Non-trainable params: 6,000\n",
      "_________________________________________________________________\n",
      "Train on 300000 samples, validate on 100000 samples\n",
      "Epoch 1/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 8.1297 - acc: 0.0139 - val_loss: 3.3827 - val_acc: 0.0200\n",
      "Epoch 2/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 1.3556 - acc: 0.0503 - val_loss: 0.7591 - val_acc: 0.0324\n",
      "Epoch 3/100\n",
      "300000/300000 [==============================] - 19s 64us/step - loss: 0.7876 - acc: 0.0825 - val_loss: 0.6585 - val_acc: 0.0529\n",
      "Epoch 4/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7844 - acc: 0.0875 - val_loss: 0.6375 - val_acc: 0.0637\n",
      "Epoch 5/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7820 - acc: 0.0918 - val_loss: 0.6343 - val_acc: 0.0665\n",
      "Epoch 6/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7812 - acc: 0.0971 - val_loss: 0.6282 - val_acc: 0.0729\n",
      "Epoch 7/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7797 - acc: 0.1008 - val_loss: 0.6136 - val_acc: 0.0777\n",
      "Epoch 8/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7784 - acc: 0.1047 - val_loss: 0.6244 - val_acc: 0.0762\n",
      "Epoch 9/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7766 - acc: 0.1072 - val_loss: 0.6204 - val_acc: 0.0790\n",
      "Epoch 10/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7757 - acc: 0.1103 - val_loss: 0.6120 - val_acc: 0.0862\n",
      "Epoch 11/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7744 - acc: 0.1140 - val_loss: 0.6030 - val_acc: 0.0913\n",
      "Epoch 12/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7715 - acc: 0.1178 - val_loss: 0.6248 - val_acc: 0.0857\n",
      "Epoch 13/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7716 - acc: 0.1200 - val_loss: 0.5776 - val_acc: 0.0973\n",
      "Epoch 14/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7697 - acc: 0.1239 - val_loss: 0.6123 - val_acc: 0.0910\n",
      "Epoch 15/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7698 - acc: 0.1273 - val_loss: 0.5754 - val_acc: 0.1025\n",
      "Epoch 16/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7686 - acc: 0.1293 - val_loss: 0.5952 - val_acc: 0.0994\n",
      "Epoch 17/100\n",
      "300000/300000 [==============================] - 20s 68us/step - loss: 0.7677 - acc: 0.1327 - val_loss: 0.6031 - val_acc: 0.1046\n",
      "Epoch 18/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7659 - acc: 0.1357 - val_loss: 0.6062 - val_acc: 0.0968\n",
      "Epoch 19/100\n",
      "300000/300000 [==============================] - 19s 64us/step - loss: 0.7664 - acc: 0.1388 - val_loss: 0.5879 - val_acc: 0.1017\n",
      "Epoch 20/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7658 - acc: 0.1421 - val_loss: 0.5651 - val_acc: 0.1058\n",
      "Epoch 21/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7652 - acc: 0.1449 - val_loss: 0.5917 - val_acc: 0.1087\n",
      "Epoch 22/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7637 - acc: 0.1472 - val_loss: 0.6046 - val_acc: 0.1057\n",
      "Epoch 23/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7626 - acc: 0.1496 - val_loss: 0.5826 - val_acc: 0.1221\n",
      "Epoch 24/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7620 - acc: 0.1541 - val_loss: 0.5941 - val_acc: 0.1111\n",
      "Epoch 25/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7624 - acc: 0.1556 - val_loss: 0.5876 - val_acc: 0.1194\n",
      "Epoch 26/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7601 - acc: 0.1579 - val_loss: 0.5755 - val_acc: 0.1242\n",
      "Epoch 27/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7595 - acc: 0.1620 - val_loss: 0.5780 - val_acc: 0.1194\n",
      "Epoch 28/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7592 - acc: 0.1616 - val_loss: 0.5981 - val_acc: 0.1224\n",
      "Epoch 29/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7592 - acc: 0.1642 - val_loss: 0.5362 - val_acc: 0.1424\n",
      "Epoch 30/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7584 - acc: 0.1661 - val_loss: 0.5397 - val_acc: 0.1391\n",
      "Epoch 31/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7592 - acc: 0.1686 - val_loss: 0.5545 - val_acc: 0.1479\n",
      "Epoch 32/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7586 - acc: 0.1698 - val_loss: 0.6096 - val_acc: 0.1335\n",
      "Epoch 33/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7585 - acc: 0.1717 - val_loss: 0.5638 - val_acc: 0.1426\n",
      "Epoch 34/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7564 - acc: 0.1742 - val_loss: 0.5334 - val_acc: 0.1368\n",
      "Epoch 35/100\n",
      "300000/300000 [==============================] - 19s 64us/step - loss: 0.7579 - acc: 0.1750 - val_loss: 0.5937 - val_acc: 0.1428\n",
      "Epoch 36/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7568 - acc: 0.1773 - val_loss: 0.5500 - val_acc: 0.1547\n",
      "Epoch 37/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7570 - acc: 0.1793 - val_loss: 0.5830 - val_acc: 0.1416\n",
      "Epoch 38/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7560 - acc: 0.1809 - val_loss: 0.5710 - val_acc: 0.1522\n",
      "Epoch 39/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7565 - acc: 0.1832 - val_loss: 0.5642 - val_acc: 0.1549\n",
      "Epoch 40/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7554 - acc: 0.1838 - val_loss: 0.5544 - val_acc: 0.1493\n",
      "Epoch 41/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7552 - acc: 0.1857 - val_loss: 0.5454 - val_acc: 0.1470\n",
      "Epoch 42/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7537 - acc: 0.1887 - val_loss: 0.5753 - val_acc: 0.1662\n",
      "Epoch 43/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7533 - acc: 0.1917 - val_loss: 0.5599 - val_acc: 0.1707\n",
      "Epoch 44/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7538 - acc: 0.1926 - val_loss: 0.5463 - val_acc: 0.1635\n",
      "Epoch 45/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7530 - acc: 0.1940 - val_loss: 0.5514 - val_acc: 0.1667\n",
      "Epoch 46/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7520 - acc: 0.1968 - val_loss: 0.5331 - val_acc: 0.1628\n",
      "Epoch 47/100\n",
      "300000/300000 [==============================] - 20s 68us/step - loss: 0.7536 - acc: 0.1982 - val_loss: 0.5891 - val_acc: 0.1521\n",
      "Epoch 48/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7524 - acc: 0.2001 - val_loss: 0.5511 - val_acc: 0.1798\n",
      "Epoch 49/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7526 - acc: 0.2017 - val_loss: 0.5794 - val_acc: 0.1705\n",
      "Epoch 50/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7513 - acc: 0.2031 - val_loss: 0.5738 - val_acc: 0.1761\n",
      "Epoch 51/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7522 - acc: 0.2047 - val_loss: 0.5821 - val_acc: 0.1656\n",
      "Epoch 52/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7522 - acc: 0.2076 - val_loss: 0.5599 - val_acc: 0.1837\n",
      "Epoch 53/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7512 - acc: 0.2087 - val_loss: 0.5443 - val_acc: 0.1614\n",
      "Epoch 54/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7504 - acc: 0.2118 - val_loss: 0.5477 - val_acc: 0.1748\n",
      "Epoch 55/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7503 - acc: 0.2131 - val_loss: 0.5274 - val_acc: 0.1540\n",
      "Epoch 56/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7513 - acc: 0.2151 - val_loss: 0.5591 - val_acc: 0.1959\n",
      "Epoch 57/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7496 - acc: 0.2175 - val_loss: 0.5292 - val_acc: 0.1805\n",
      "Epoch 58/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7497 - acc: 0.2187 - val_loss: 0.5502 - val_acc: 0.1812\n",
      "Epoch 59/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7512 - acc: 0.2201 - val_loss: 0.5743 - val_acc: 0.1789\n",
      "Epoch 60/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7499 - acc: 0.2217 - val_loss: 0.5772 - val_acc: 0.1989\n",
      "Epoch 61/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7501 - acc: 0.2237 - val_loss: 0.5409 - val_acc: 0.2022\n",
      "Epoch 62/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7490 - acc: 0.2243 - val_loss: 0.5479 - val_acc: 0.2007\n",
      "Epoch 63/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7485 - acc: 0.2261 - val_loss: 0.5970 - val_acc: 0.1805\n",
      "Epoch 64/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7485 - acc: 0.2285 - val_loss: 0.5872 - val_acc: 0.2107\n",
      "Epoch 65/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7491 - acc: 0.2293 - val_loss: 0.5648 - val_acc: 0.1860\n",
      "Epoch 66/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7496 - acc: 0.2308 - val_loss: 0.5629 - val_acc: 0.1983\n",
      "Epoch 67/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7486 - acc: 0.2337 - val_loss: 0.5687 - val_acc: 0.2038\n",
      "Epoch 68/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7482 - acc: 0.2355 - val_loss: 0.5957 - val_acc: 0.1952\n",
      "Epoch 69/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7479 - acc: 0.2352 - val_loss: 0.5516 - val_acc: 0.1897\n",
      "Epoch 70/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7489 - acc: 0.2358 - val_loss: 0.5451 - val_acc: 0.2194\n",
      "Epoch 71/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7479 - acc: 0.2383 - val_loss: 0.5656 - val_acc: 0.2233\n",
      "Epoch 72/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7482 - acc: 0.2400 - val_loss: 0.5199 - val_acc: 0.2123\n",
      "Epoch 73/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7483 - acc: 0.2399 - val_loss: 0.5266 - val_acc: 0.2134\n",
      "Epoch 74/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7462 - acc: 0.2420 - val_loss: 0.5670 - val_acc: 0.2033\n",
      "Epoch 75/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7484 - acc: 0.2413 - val_loss: 0.5496 - val_acc: 0.2349\n",
      "Epoch 76/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7475 - acc: 0.2435 - val_loss: 0.5467 - val_acc: 0.2207\n",
      "Epoch 77/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7484 - acc: 0.2457 - val_loss: 0.5460 - val_acc: 0.1685\n",
      "Epoch 78/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7480 - acc: 0.2467 - val_loss: 0.5680 - val_acc: 0.2280\n",
      "Epoch 79/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7475 - acc: 0.2465 - val_loss: 0.5900 - val_acc: 0.2179\n",
      "Epoch 80/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7470 - acc: 0.2469 - val_loss: 0.5702 - val_acc: 0.2152\n",
      "Epoch 81/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7469 - acc: 0.2487 - val_loss: 0.5498 - val_acc: 0.2066\n",
      "Epoch 82/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7480 - acc: 0.2496 - val_loss: 0.5735 - val_acc: 0.2231\n",
      "Epoch 83/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7476 - acc: 0.2502 - val_loss: 0.5575 - val_acc: 0.2133\n",
      "Epoch 84/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7471 - acc: 0.2509 - val_loss: 0.5388 - val_acc: 0.1830\n",
      "Epoch 85/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7461 - acc: 0.2517 - val_loss: 0.5513 - val_acc: 0.2298\n",
      "Epoch 00085: early stopping\n",
      "100000/100000 [==============================] - 5s 49us/step\n",
      "400000/400000 [==============================] - 18s 44us/step\n",
      "fold 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1500)              301500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1500)              6000      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1500)              6000      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 200)               300200    \n",
      "=================================================================\n",
      "Total params: 5,116,700\n",
      "Trainable params: 5,110,700\n",
      "Non-trainable params: 6,000\n",
      "_________________________________________________________________\n",
      "Train on 300000 samples, validate on 100000 samples\n",
      "Epoch 1/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 8.1456 - acc: 0.0140 - val_loss: 3.4347 - val_acc: 0.0195\n",
      "Epoch 2/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 1.3701 - acc: 0.0504 - val_loss: 0.7615 - val_acc: 0.0259\n",
      "Epoch 3/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7801 - acc: 0.0819 - val_loss: 0.6586 - val_acc: 0.0578\n",
      "Epoch 4/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7773 - acc: 0.0883 - val_loss: 0.6376 - val_acc: 0.0574\n",
      "Epoch 5/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7749 - acc: 0.0929 - val_loss: 0.6374 - val_acc: 0.0682\n",
      "Epoch 6/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7741 - acc: 0.0973 - val_loss: 0.6328 - val_acc: 0.0722\n",
      "Epoch 7/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7725 - acc: 0.1009 - val_loss: 0.6290 - val_acc: 0.0764\n",
      "Epoch 8/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7708 - acc: 0.1042 - val_loss: 0.6242 - val_acc: 0.0794\n",
      "Epoch 9/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7687 - acc: 0.1072 - val_loss: 0.6136 - val_acc: 0.0885\n",
      "Epoch 10/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7681 - acc: 0.1101 - val_loss: 0.6076 - val_acc: 0.0800\n",
      "Epoch 11/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7672 - acc: 0.1145 - val_loss: 0.6089 - val_acc: 0.0937\n",
      "Epoch 12/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7664 - acc: 0.1170 - val_loss: 0.6115 - val_acc: 0.0870\n",
      "Epoch 13/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7648 - acc: 0.1195 - val_loss: 0.5928 - val_acc: 0.0898\n",
      "Epoch 14/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7634 - acc: 0.1233 - val_loss: 0.5966 - val_acc: 0.0927\n",
      "Epoch 15/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7625 - acc: 0.1253 - val_loss: 0.6041 - val_acc: 0.1000\n",
      "Epoch 16/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7612 - acc: 0.1289 - val_loss: 0.6245 - val_acc: 0.0942\n",
      "Epoch 17/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7608 - acc: 0.1317 - val_loss: 0.5784 - val_acc: 0.1011\n",
      "Epoch 18/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7604 - acc: 0.1360 - val_loss: 0.5548 - val_acc: 0.1058\n",
      "Epoch 19/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7592 - acc: 0.1388 - val_loss: 0.5792 - val_acc: 0.1094\n",
      "Epoch 20/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7572 - acc: 0.1398 - val_loss: 0.6123 - val_acc: 0.1072\n",
      "Epoch 21/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7566 - acc: 0.1452 - val_loss: 0.5831 - val_acc: 0.1130\n",
      "Epoch 22/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7569 - acc: 0.1475 - val_loss: 0.5794 - val_acc: 0.1089\n",
      "Epoch 23/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7558 - acc: 0.1496 - val_loss: 0.5612 - val_acc: 0.1228\n",
      "Epoch 24/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7546 - acc: 0.1523 - val_loss: 0.5671 - val_acc: 0.1279\n",
      "Epoch 25/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7543 - acc: 0.1547 - val_loss: 0.5878 - val_acc: 0.1226\n",
      "Epoch 26/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7533 - acc: 0.1565 - val_loss: 0.5698 - val_acc: 0.1267\n",
      "Epoch 27/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7537 - acc: 0.1576 - val_loss: 0.5449 - val_acc: 0.1271\n",
      "Epoch 28/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7522 - acc: 0.1618 - val_loss: 0.5551 - val_acc: 0.1179\n",
      "Epoch 29/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7529 - acc: 0.1636 - val_loss: 0.5430 - val_acc: 0.1315\n",
      "Epoch 30/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7530 - acc: 0.1639 - val_loss: 0.5687 - val_acc: 0.1245\n",
      "Epoch 31/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7522 - acc: 0.1658 - val_loss: 0.5698 - val_acc: 0.1333\n",
      "Epoch 32/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7508 - acc: 0.1678 - val_loss: 0.5513 - val_acc: 0.1448\n",
      "Epoch 33/100\n",
      "300000/300000 [==============================] - 20s 68us/step - loss: 0.7506 - acc: 0.1694 - val_loss: 0.5429 - val_acc: 0.1280\n",
      "Epoch 34/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7509 - acc: 0.1724 - val_loss: 0.5457 - val_acc: 0.1345\n",
      "Epoch 35/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7507 - acc: 0.1723 - val_loss: 0.5647 - val_acc: 0.1458\n",
      "Epoch 36/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7509 - acc: 0.1732 - val_loss: 0.5462 - val_acc: 0.1438\n",
      "Epoch 37/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7498 - acc: 0.1755 - val_loss: 0.5468 - val_acc: 0.1411\n",
      "Epoch 38/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7494 - acc: 0.1771 - val_loss: 0.5776 - val_acc: 0.1395\n",
      "Epoch 39/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7482 - acc: 0.1789 - val_loss: 0.5387 - val_acc: 0.1398\n",
      "Epoch 40/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7489 - acc: 0.1794 - val_loss: 0.5519 - val_acc: 0.1521\n",
      "Epoch 41/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7476 - acc: 0.1802 - val_loss: 0.5334 - val_acc: 0.1493\n",
      "Epoch 42/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7487 - acc: 0.1817 - val_loss: 0.5754 - val_acc: 0.1498\n",
      "Epoch 43/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7472 - acc: 0.1831 - val_loss: 0.5631 - val_acc: 0.1546\n",
      "Epoch 44/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7476 - acc: 0.1842 - val_loss: 0.5482 - val_acc: 0.1587\n",
      "Epoch 45/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7477 - acc: 0.1845 - val_loss: 0.5626 - val_acc: 0.1449\n",
      "Epoch 46/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7465 - acc: 0.1868 - val_loss: 0.5533 - val_acc: 0.1555\n",
      "Epoch 47/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7468 - acc: 0.1873 - val_loss: 0.5473 - val_acc: 0.1659\n",
      "Epoch 48/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7461 - acc: 0.1887 - val_loss: 0.5572 - val_acc: 0.1695\n",
      "Epoch 49/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7458 - acc: 0.1890 - val_loss: 0.5665 - val_acc: 0.1445\n",
      "Epoch 50/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7463 - acc: 0.1909 - val_loss: 0.5624 - val_acc: 0.1692\n",
      "Epoch 51/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7463 - acc: 0.1924 - val_loss: 0.5899 - val_acc: 0.1546\n",
      "Epoch 52/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7465 - acc: 0.1934 - val_loss: 0.5383 - val_acc: 0.1681\n",
      "Epoch 53/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7449 - acc: 0.1933 - val_loss: 0.5520 - val_acc: 0.1645\n",
      "Epoch 54/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7445 - acc: 0.1948 - val_loss: 0.5693 - val_acc: 0.1660\n",
      "Epoch 55/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7453 - acc: 0.1967 - val_loss: 0.5067 - val_acc: 0.1679\n",
      "Epoch 56/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7445 - acc: 0.1973 - val_loss: 0.5696 - val_acc: 0.1531\n",
      "Epoch 57/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7449 - acc: 0.1992 - val_loss: 0.5532 - val_acc: 0.1571\n",
      "Epoch 58/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7439 - acc: 0.2011 - val_loss: 0.5708 - val_acc: 0.1636\n",
      "Epoch 00058: early stopping\n",
      "100000/100000 [==============================] - 5s 48us/step\n",
      "400000/400000 [==============================] - 19s 49us/step\n",
      "fold 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1500)              301500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1500)              6000      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1500)              6000      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 200)               300200    \n",
      "=================================================================\n",
      "Total params: 5,116,700\n",
      "Trainable params: 5,110,700\n",
      "Non-trainable params: 6,000\n",
      "_________________________________________________________________\n",
      "Train on 300000 samples, validate on 100000 samples\n",
      "Epoch 1/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 8.1324 - acc: 0.0147 - val_loss: 3.4231 - val_acc: 0.0194\n",
      "Epoch 2/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 1.3696 - acc: 0.0496 - val_loss: 0.7616 - val_acc: 0.0389\n",
      "Epoch 3/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7866 - acc: 0.0817 - val_loss: 0.6870 - val_acc: 0.0468\n",
      "Epoch 4/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7839 - acc: 0.0881 - val_loss: 0.6859 - val_acc: 0.0589\n",
      "Epoch 5/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7814 - acc: 0.0938 - val_loss: 0.6298 - val_acc: 0.0628\n",
      "Epoch 6/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7798 - acc: 0.0969 - val_loss: 0.6279 - val_acc: 0.0656\n",
      "Epoch 7/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7791 - acc: 0.1017 - val_loss: 0.6288 - val_acc: 0.0728\n",
      "Epoch 8/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7778 - acc: 0.1059 - val_loss: 0.6425 - val_acc: 0.0758\n",
      "Epoch 9/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7759 - acc: 0.1083 - val_loss: 0.6299 - val_acc: 0.0770\n",
      "Epoch 10/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7751 - acc: 0.1116 - val_loss: 0.6140 - val_acc: 0.0855\n",
      "Epoch 11/100\n",
      "300000/300000 [==============================] - 20s 68us/step - loss: 0.7740 - acc: 0.1144 - val_loss: 0.6174 - val_acc: 0.0867\n",
      "Epoch 12/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7708 - acc: 0.1174 - val_loss: 0.6214 - val_acc: 0.0854\n",
      "Epoch 13/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7720 - acc: 0.1215 - val_loss: 0.6191 - val_acc: 0.0889\n",
      "Epoch 14/100\n",
      "300000/300000 [==============================] - 19s 64us/step - loss: 0.7691 - acc: 0.1249 - val_loss: 0.6092 - val_acc: 0.0968\n",
      "Epoch 15/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7688 - acc: 0.1284 - val_loss: 0.5895 - val_acc: 0.1020\n",
      "Epoch 16/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7674 - acc: 0.1298 - val_loss: 0.5698 - val_acc: 0.1050\n",
      "Epoch 17/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7668 - acc: 0.1352 - val_loss: 0.6032 - val_acc: 0.0953\n",
      "Epoch 18/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7655 - acc: 0.1373 - val_loss: 0.5793 - val_acc: 0.1099\n",
      "Epoch 19/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7657 - acc: 0.1406 - val_loss: 0.6180 - val_acc: 0.1056\n",
      "Epoch 20/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7641 - acc: 0.1431 - val_loss: 0.6034 - val_acc: 0.1099\n",
      "Epoch 21/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7634 - acc: 0.1471 - val_loss: 0.6185 - val_acc: 0.1077\n",
      "Epoch 22/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7635 - acc: 0.1507 - val_loss: 0.5976 - val_acc: 0.1143\n",
      "Epoch 23/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7619 - acc: 0.1527 - val_loss: 0.5803 - val_acc: 0.1126\n",
      "Epoch 24/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7618 - acc: 0.1538 - val_loss: 0.5703 - val_acc: 0.1162\n",
      "Epoch 25/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7607 - acc: 0.1558 - val_loss: 0.6263 - val_acc: 0.1161\n",
      "Epoch 26/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7601 - acc: 0.1592 - val_loss: 0.5753 - val_acc: 0.1283\n",
      "Epoch 27/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7602 - acc: 0.1624 - val_loss: 0.5719 - val_acc: 0.1323\n",
      "Epoch 28/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7586 - acc: 0.1634 - val_loss: 0.5762 - val_acc: 0.1171\n",
      "Epoch 29/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7585 - acc: 0.1651 - val_loss: 0.5877 - val_acc: 0.1175\n",
      "Epoch 30/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7574 - acc: 0.1665 - val_loss: 0.5491 - val_acc: 0.1349\n",
      "Epoch 31/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7579 - acc: 0.1690 - val_loss: 0.5642 - val_acc: 0.1239\n",
      "Epoch 32/100\n",
      "300000/300000 [==============================] - 20s 68us/step - loss: 0.7565 - acc: 0.1701 - val_loss: 0.5871 - val_acc: 0.1295\n",
      "Epoch 33/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7587 - acc: 0.1729 - val_loss: 0.5393 - val_acc: 0.1380\n",
      "Epoch 34/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7563 - acc: 0.1741 - val_loss: 0.5812 - val_acc: 0.1393\n",
      "Epoch 35/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7556 - acc: 0.1736 - val_loss: 0.5831 - val_acc: 0.1403\n",
      "Epoch 36/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7556 - acc: 0.1782 - val_loss: 0.6115 - val_acc: 0.1394\n",
      "Epoch 37/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7556 - acc: 0.1781 - val_loss: 0.5544 - val_acc: 0.1284\n",
      "Epoch 38/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7548 - acc: 0.1805 - val_loss: 0.5850 - val_acc: 0.1359\n",
      "Epoch 39/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7548 - acc: 0.1814 - val_loss: 0.5613 - val_acc: 0.1502\n",
      "Epoch 40/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7552 - acc: 0.1834 - val_loss: 0.5813 - val_acc: 0.1399\n",
      "Epoch 41/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7535 - acc: 0.1858 - val_loss: 0.5486 - val_acc: 0.1461\n",
      "Epoch 42/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7529 - acc: 0.1849 - val_loss: 0.5546 - val_acc: 0.1550\n",
      "Epoch 43/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7526 - acc: 0.1891 - val_loss: 0.5408 - val_acc: 0.1649\n",
      "Epoch 44/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7534 - acc: 0.1894 - val_loss: 0.5821 - val_acc: 0.1437\n",
      "Epoch 45/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7518 - acc: 0.1915 - val_loss: 0.5057 - val_acc: 0.1598\n",
      "Epoch 46/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7526 - acc: 0.1923 - val_loss: 0.6131 - val_acc: 0.1493\n",
      "Epoch 47/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7522 - acc: 0.1939 - val_loss: 0.5758 - val_acc: 0.1611\n",
      "Epoch 48/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7506 - acc: 0.1968 - val_loss: 0.5474 - val_acc: 0.1586\n",
      "Epoch 49/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7521 - acc: 0.1981 - val_loss: 0.5506 - val_acc: 0.1788\n",
      "Epoch 50/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7510 - acc: 0.1990 - val_loss: 0.5607 - val_acc: 0.1507\n",
      "Epoch 51/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7508 - acc: 0.2019 - val_loss: 0.5676 - val_acc: 0.1689\n",
      "Epoch 52/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7511 - acc: 0.2029 - val_loss: 0.5267 - val_acc: 0.1887\n",
      "Epoch 53/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7502 - acc: 0.2045 - val_loss: 0.5316 - val_acc: 0.1580\n",
      "Epoch 54/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7507 - acc: 0.2075 - val_loss: 0.5567 - val_acc: 0.1671\n",
      "Epoch 55/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7507 - acc: 0.2082 - val_loss: 0.5277 - val_acc: 0.1869\n",
      "Epoch 56/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7502 - acc: 0.2100 - val_loss: 0.5758 - val_acc: 0.1643\n",
      "Epoch 57/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7505 - acc: 0.2116 - val_loss: 0.5576 - val_acc: 0.1737\n",
      "Epoch 58/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7490 - acc: 0.2138 - val_loss: 0.5729 - val_acc: 0.1743\n",
      "Epoch 59/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7490 - acc: 0.2163 - val_loss: 0.5672 - val_acc: 0.1729\n",
      "Epoch 60/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7489 - acc: 0.2163 - val_loss: 0.5443 - val_acc: 0.1910\n",
      "Epoch 61/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7501 - acc: 0.2185 - val_loss: 0.5624 - val_acc: 0.1992\n",
      "Epoch 62/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7493 - acc: 0.2207 - val_loss: 0.5216 - val_acc: 0.1832\n",
      "Epoch 63/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7486 - acc: 0.2222 - val_loss: 0.5568 - val_acc: 0.1995\n",
      "Epoch 64/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7478 - acc: 0.2239 - val_loss: 0.5520 - val_acc: 0.1857\n",
      "Epoch 65/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7492 - acc: 0.2256 - val_loss: 0.5538 - val_acc: 0.1848\n",
      "Epoch 66/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7489 - acc: 0.2266 - val_loss: 0.5480 - val_acc: 0.1894\n",
      "Epoch 67/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7490 - acc: 0.2288 - val_loss: 0.5266 - val_acc: 0.2005\n",
      "Epoch 68/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7464 - acc: 0.2304 - val_loss: 0.5424 - val_acc: 0.1831\n",
      "Epoch 69/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7471 - acc: 0.2318 - val_loss: 0.5817 - val_acc: 0.1916\n",
      "Epoch 70/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7479 - acc: 0.2315 - val_loss: 0.5470 - val_acc: 0.2024\n",
      "Epoch 71/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7470 - acc: 0.2335 - val_loss: 0.5257 - val_acc: 0.1721\n",
      "Epoch 72/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7481 - acc: 0.2357 - val_loss: 0.5560 - val_acc: 0.1930\n",
      "Epoch 73/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7466 - acc: 0.2368 - val_loss: 0.5255 - val_acc: 0.2059\n",
      "Epoch 74/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7472 - acc: 0.2377 - val_loss: 0.5470 - val_acc: 0.2070\n",
      "Epoch 75/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7466 - acc: 0.2377 - val_loss: 0.5826 - val_acc: 0.1773\n",
      "Epoch 76/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7470 - acc: 0.2402 - val_loss: 0.5856 - val_acc: 0.2056\n",
      "Epoch 77/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7469 - acc: 0.2395 - val_loss: 0.5722 - val_acc: 0.2008\n",
      "Epoch 78/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7476 - acc: 0.2419 - val_loss: 0.5867 - val_acc: 0.1897\n",
      "Epoch 79/100\n",
      "300000/300000 [==============================] - 19s 65us/step - loss: 0.7470 - acc: 0.2439 - val_loss: 0.5533 - val_acc: 0.2103\n",
      "Epoch 80/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7459 - acc: 0.2428 - val_loss: 0.5735 - val_acc: 0.2039\n",
      "Epoch 81/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7470 - acc: 0.2455 - val_loss: 0.5552 - val_acc: 0.2151\n",
      "Epoch 82/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7468 - acc: 0.2464 - val_loss: 0.5324 - val_acc: 0.2212\n",
      "Epoch 83/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7467 - acc: 0.2478 - val_loss: 0.5362 - val_acc: 0.2072\n",
      "Epoch 84/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7469 - acc: 0.2477 - val_loss: 0.5442 - val_acc: 0.2219\n",
      "Epoch 85/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7474 - acc: 0.2478 - val_loss: 0.5445 - val_acc: 0.2174\n",
      "Epoch 86/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7466 - acc: 0.2487 - val_loss: 0.5477 - val_acc: 0.1942\n",
      "Epoch 87/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7458 - acc: 0.2489 - val_loss: 0.5389 - val_acc: 0.1882\n",
      "Epoch 88/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7474 - acc: 0.2490 - val_loss: 0.5222 - val_acc: 0.2049\n",
      "Epoch 89/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7451 - acc: 0.2515 - val_loss: 0.5604 - val_acc: 0.2027\n",
      "Epoch 90/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7466 - acc: 0.2510 - val_loss: 0.5269 - val_acc: 0.1947\n",
      "Epoch 91/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7439 - acc: 0.2520 - val_loss: 0.5186 - val_acc: 0.2357\n",
      "Epoch 92/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7454 - acc: 0.2552 - val_loss: 0.5759 - val_acc: 0.2220\n",
      "Epoch 93/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7453 - acc: 0.2545 - val_loss: 0.5697 - val_acc: 0.1980\n",
      "Epoch 94/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7461 - acc: 0.2541 - val_loss: 0.5469 - val_acc: 0.2241\n",
      "Epoch 95/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7449 - acc: 0.2549 - val_loss: 0.5729 - val_acc: 0.2321\n",
      "Epoch 96/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7472 - acc: 0.2541 - val_loss: 0.5624 - val_acc: 0.2295\n",
      "Epoch 97/100\n",
      "300000/300000 [==============================] - 20s 65us/step - loss: 0.7458 - acc: 0.2575 - val_loss: 0.5576 - val_acc: 0.1765\n",
      "Epoch 98/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7472 - acc: 0.2572 - val_loss: 0.5253 - val_acc: 0.2248\n",
      "Epoch 99/100\n",
      "300000/300000 [==============================] - 20s 67us/step - loss: 0.7456 - acc: 0.2578 - val_loss: 0.5468 - val_acc: 0.1945\n",
      "Epoch 100/100\n",
      "300000/300000 [==============================] - 20s 66us/step - loss: 0.7461 - acc: 0.2589 - val_loss: 0.5197 - val_acc: 0.2081\n",
      "100000/100000 [==============================] - 5s 49us/step\n",
      "400000/400000 [==============================] - 19s 48us/step\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.activations import elu\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras.objectives import binary_crossentropy\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from imblearn.keras import balanced_batch_generator\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler, CondensedNearestNeighbour, AllKNN, InstanceHardnessThreshold\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "learning_rate = 0.0003\n",
    "mom = 0.2\n",
    "dcy = 0.996\n",
    "nb_folds = 4\n",
    "nb_epoch = 100\n",
    "batch_size = 140\n",
    "encoding_dim =1500\n",
    "hidden_dim = int(encoding_dim) #i.e. 7\n",
    "sgd = SGD(lr=learning_rate, momentum=mom, decay=dcy)\n",
    "#folds = StratifiedKFold(n_splits=nb_folds, shuffle=True, random_state=420)\n",
    "folds = KFold(n_splits = nb_folds, random_state = 338, shuffle = True)\n",
    "auto = np.zeros(df[trafo_columns].shape)\n",
    "layer_output = np.zeros((len(df), 1500)) # change when nn shape changes\n",
    "#layer_output = np.zeros(df[trafo_columns].shape)\n",
    "#train_auto = np.zeros(train[trafo_columns].shape)\n",
    "#test_auto = np.zeros(test[trafo_columns].shape)\n",
    "predictions = np.zeros(len(df))\n",
    "#label_cols = [\"target\"]\n",
    "#y_split = train[label_cols].values\n",
    "\n",
    "# Horovod: adjust learning rate based on number of GPUs.\n",
    "opt = keras.optimizers.SGD(lr=learning_rate, decay=dcy, momentum=mom, nesterov=True)\n",
    "# Horovod: add Horovod Distributed Optimizer.\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "cp = ModelCheckpoint(filepath=\"autoencoder_0.h5\",\n",
    "                               save_best_only=True,\n",
    "                               verbose=0)\n",
    "\n",
    "tb = TensorBoard(log_dir='./logs',\n",
    "                histogram_freq=0,\n",
    "                write_graph=True,\n",
    "                write_images=True)\n",
    "\n",
    "es= EarlyStopping(monitor='val_acc',\n",
    "                  min_delta=0,\n",
    "                  patience=20,\n",
    "                  verbose=1, mode='min')\n",
    "\n",
    "#for fold_, (trn_idx, val_idx) in enumerate(folds.split(y_split[:,0], y_split[:,0])):\n",
    "#    print(\"fold {}\".format(fold_))\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "\n",
    "    trn_data = df[trafo_columns].iloc[trn_idx]\n",
    "    val_data = df[trafo_columns].iloc[val_idx]\n",
    "\n",
    "    def add_noise(series, noise_level):\n",
    "        return series * (1 + noise_level * np.random.randn(series.shape[1]))\n",
    "    \n",
    "    noisy_trn_data = add_noise(trn_data, 0.04)\n",
    "\n",
    "    input_dim = noisy_trn_data.shape[1] #num of columns, 30\n",
    "    input_layer = Input(shape=(input_dim, ))\n",
    "    encoder = Dense(encoding_dim, activation=\"tanh\", activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Dense(hidden_dim, activation=\"relu\")(encoder)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    decoder = Dense(hidden_dim, activation='relu')(encoder)\n",
    "    decoder = Dense(input_dim, activation='tanh')(decoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    autoencoder.summary()\n",
    "    \n",
    "    autoencoder.compile(metrics=['accuracy'],\n",
    "                        loss='mean_squared_error',\n",
    "                        optimizer='sgd')\n",
    "\n",
    "    cp = ModelCheckpoint(filepath=\"autoencoder_fraud.h5\",\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=0)\n",
    "\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=True)\n",
    "    \n",
    "    es= EarlyStopping(monitor='val_acc',\n",
    "                  min_delta=0,\n",
    "                  patience=10,\n",
    "                  verbose=1, mode='auto')\n",
    "\n",
    "    history = autoencoder.fit(noisy_trn_data, trn_data,\n",
    "                        epochs=nb_epoch,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(val_data, val_data),\n",
    "                        verbose=1,\n",
    "                        callbacks=[cp, tb, es]).history\n",
    "    \n",
    "    \n",
    "    auto[val_idx] += autoencoder.predict(df.iloc[val_idx][trafo_columns], verbose=1)\n",
    "    mse = autoencoder.predict(df[trafo_columns] / folds.n_splits, verbose=1)\n",
    "    predictions += np.mean(np.power(df[trafo_columns] - mse, 2), axis=1)\n",
    "    # we build a new model with the activations of the old model\n",
    "    # this model is truncated after the first layer\n",
    "    get_1st_layer_output = K.function([autoencoder.layers[0].input],\n",
    "                                  [autoencoder.layers[1].output])\n",
    "    layer_output[val_idx] += pd.DataFrame(np.concatenate(get_1st_layer_output([df.iloc[val_idx][trafo_columns]])))\n",
    "    \n",
    "auto_final = pd.DataFrame(auto / folds.n_splits)\n",
    "hidden = pd.DataFrame(layer_output / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>target</th>\n",
       "      <th>ID_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.037098</td>\n",
       "      <td>-0.246259</td>\n",
       "      <td>0.161921</td>\n",
       "      <td>0.113324</td>\n",
       "      <td>0.109084</td>\n",
       "      <td>-0.227911</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>0.170681</td>\n",
       "      <td>-0.204606</td>\n",
       "      <td>-0.160791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233470</td>\n",
       "      <td>0.127236</td>\n",
       "      <td>0.102966</td>\n",
       "      <td>-0.207186</td>\n",
       "      <td>0.208276</td>\n",
       "      <td>0.066652</td>\n",
       "      <td>-0.016748</td>\n",
       "      <td>-0.201666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049664</td>\n",
       "      <td>-0.236835</td>\n",
       "      <td>0.141943</td>\n",
       "      <td>0.095345</td>\n",
       "      <td>0.099269</td>\n",
       "      <td>0.188610</td>\n",
       "      <td>0.047097</td>\n",
       "      <td>0.006559</td>\n",
       "      <td>0.243091</td>\n",
       "      <td>0.164853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241266</td>\n",
       "      <td>0.237709</td>\n",
       "      <td>-0.021244</td>\n",
       "      <td>0.240099</td>\n",
       "      <td>0.229344</td>\n",
       "      <td>0.072282</td>\n",
       "      <td>0.083841</td>\n",
       "      <td>0.192511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.115840</td>\n",
       "      <td>-0.209504</td>\n",
       "      <td>0.185729</td>\n",
       "      <td>0.172690</td>\n",
       "      <td>0.040462</td>\n",
       "      <td>-0.242156</td>\n",
       "      <td>0.173452</td>\n",
       "      <td>-0.095028</td>\n",
       "      <td>-0.189040</td>\n",
       "      <td>-0.041403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239066</td>\n",
       "      <td>0.208457</td>\n",
       "      <td>0.174665</td>\n",
       "      <td>0.246209</td>\n",
       "      <td>-0.241916</td>\n",
       "      <td>0.029830</td>\n",
       "      <td>-0.000666</td>\n",
       "      <td>0.056036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147483</td>\n",
       "      <td>-0.206408</td>\n",
       "      <td>0.021531</td>\n",
       "      <td>0.128069</td>\n",
       "      <td>0.142741</td>\n",
       "      <td>-0.172584</td>\n",
       "      <td>0.084944</td>\n",
       "      <td>0.025649</td>\n",
       "      <td>-0.223320</td>\n",
       "      <td>0.156077</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177676</td>\n",
       "      <td>0.200196</td>\n",
       "      <td>0.172018</td>\n",
       "      <td>-0.199926</td>\n",
       "      <td>-0.222900</td>\n",
       "      <td>0.133089</td>\n",
       "      <td>0.133401</td>\n",
       "      <td>-0.229188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.105962</td>\n",
       "      <td>-0.222930</td>\n",
       "      <td>0.163316</td>\n",
       "      <td>0.110949</td>\n",
       "      <td>0.146369</td>\n",
       "      <td>0.101697</td>\n",
       "      <td>0.109621</td>\n",
       "      <td>0.110542</td>\n",
       "      <td>0.244619</td>\n",
       "      <td>0.073885</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095144</td>\n",
       "      <td>0.230949</td>\n",
       "      <td>-0.115674</td>\n",
       "      <td>-0.208460</td>\n",
       "      <td>0.223600</td>\n",
       "      <td>0.136776</td>\n",
       "      <td>0.172106</td>\n",
       "      <td>-0.239442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.037098 -0.246259  0.161921  0.113324  0.109084 -0.227911  0.037021   \n",
       "1  0.049664 -0.236835  0.141943  0.095345  0.099269  0.188610  0.047097   \n",
       "2  0.115840 -0.209504  0.185729  0.172690  0.040462 -0.242156  0.173452   \n",
       "3  0.147483 -0.206408  0.021531  0.128069  0.142741 -0.172584  0.084944   \n",
       "4  0.105962 -0.222930  0.163316  0.110949  0.146369  0.101697  0.109621   \n",
       "\n",
       "          7         8         9   ...          192       193       194  \\\n",
       "0  0.170681 -0.204606 -0.160791   ...     0.233470  0.127236  0.102966   \n",
       "1  0.006559  0.243091  0.164853   ...     0.241266  0.237709 -0.021244   \n",
       "2 -0.095028 -0.189040 -0.041403   ...     0.239066  0.208457  0.174665   \n",
       "3  0.025649 -0.223320  0.156077   ...     0.177676  0.200196  0.172018   \n",
       "4  0.110542  0.244619  0.073885   ...    -0.095144  0.230949 -0.115674   \n",
       "\n",
       "        195       196       197       198       199  target  ID_code  \n",
       "0 -0.207186  0.208276  0.066652 -0.016748 -0.201666     0.0  train_0  \n",
       "1  0.240099  0.229344  0.072282  0.083841  0.192511     0.0  train_1  \n",
       "2  0.246209 -0.241916  0.029830 -0.000666  0.056036     0.0  train_2  \n",
       "3 -0.199926 -0.222900  0.133089  0.133401 -0.229188     0.0  train_3  \n",
       "4 -0.208460  0.223600  0.136776  0.172106 -0.239442     0.0  train_4  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hidden\n",
    "hidden['target'] = target\n",
    "hidden['ID_code'] = id.values\n",
    "hidden.head(5)\n",
    "#final\n",
    "auto_final['target'] = target\n",
    "auto_final['ID_code'] = id.values\n",
    "auto_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable         Type         Data/Info\n",
      "---------------------------------------\n",
      "auto_final       DataFrame                   0         <...>00000 rows x 202 columns]\n",
      "df               DataFrame               var_0     var_<...>00000 rows x 201 columns]\n",
      "df_original      DataFrame                ID_code  targ<...>00000 rows x 202 columns]\n",
      "hidden           DataFrame                   0         <...>0000 rows x 1502 columns]\n",
      "noisy_trn_data   DataFrame               var_0     var_<...>00000 rows x 200 columns]\n",
      "test             DataFrame                ID_code      <...>00000 rows x 201 columns]\n",
      "train            DataFrame                 ID_code  tar<...>00000 rows x 202 columns]\n",
      "trn_data         DataFrame               var_0     var_<...>00000 rows x 200 columns]\n",
      "val_data         DataFrame               var_0     var_<...>00000 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "%whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "172"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df, noisy_trn_data, test, train, trn_data, val_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable      Type         Data/Info\n",
      "------------------------------------\n",
      "auto_final    DataFrame                   0         <...>00000 rows x 202 columns]\n",
      "df_original   DataFrame                ID_code  targ<...>00000 rows x 202 columns]\n",
      "hidden        DataFrame                   0         <...>0000 rows x 1502 columns]\n"
     ]
    }
   ],
   "source": [
    "%whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 1502)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hidden\n",
    "ae_columns = [c for c in hidden.columns if c not in ['ID_code', 'target']]\n",
    "df_train = hidden[hidden['target'].notnull()]\n",
    "target = df_train['target']\n",
    "df_test = hidden[hidden['target'].isnull()]\n",
    "df_train.shape\n",
    "\n",
    "#final\n",
    "#ae_columns = [c for c in auto_final.columns if c not in ['ID_code', 'target']]\n",
    "#df_train = auto_final[auto_final['target'].notnull()]\n",
    "#target = df_train['target']\n",
    "#df_test = auto_final[auto_final['target'].isnull()]\n",
    "#df_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Fitting 3 folds for each of 20000 candidates, totalling 60000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/wopr/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/wopr/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid's auc: 0.782865\n",
      "[200]\tvalid's auc: 0.810072\n",
      "[300]\tvalid's auc: 0.821876\n",
      "[400]\tvalid's auc: 0.827531\n",
      "[500]\tvalid's auc: 0.830813\n",
      "[600]\tvalid's auc: 0.832416\n",
      "[700]\tvalid's auc: 0.833096\n",
      "[800]\tvalid's auc: 0.833342\n",
      "[900]\tvalid's auc: 0.833351\n",
      "[1000]\tvalid's auc: 0.833519\n",
      "[1100]\tvalid's auc: 0.833789\n",
      "[1200]\tvalid's auc: 0.834354\n",
      "[1300]\tvalid's auc: 0.834716\n",
      "[1400]\tvalid's auc: 0.834623\n",
      "Early stopping, best iteration is:\n",
      "[1325]\tvalid's auc: 0.834793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wopr/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/wopr/.local/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "label_cols = [\"target\"]\n",
    "y_split = df_train[label_cols].values\n",
    "\n",
    "param_test ={'num_leaves': sp_randint(6, 50), \n",
    "             'min_child_samples': sp_randint(100, 500), \n",
    "             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "             'subsample': sp_uniform(loc=0.2, scale=0.8), \n",
    "             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(y_split[:,0], y_split[:,0])):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "\n",
    "    train_x, train_y = df_train[ae_columns].iloc[trn_idx], df_train['target'].iloc[trn_idx]\n",
    "#    trn_data, trn_y = df_train[ae_columns].iloc[trn_idx], df_train['target'].iloc[trn_idx]\n",
    "    val_x, val_y = df_train[ae_columns].iloc[val_idx], df_train['target'].iloc[val_idx]\n",
    "\n",
    "#    classes=[]\n",
    "#    for i in np.unique(trn_y):\n",
    "#        classes.append(i)\n",
    "#        print(\"Before OverSampling, counts of label \" + str(i) + \": {}\".format(sum(trn_y==i)))\n",
    "\n",
    "#    sm=SMOTE(random_state=2)\n",
    "#    train_x, train_y = sm.fit_sample(trn_data, trn_y.ravel())\n",
    "\n",
    "#    print('After OverSampling, the shape of train_X: {}'.format(train_x.shape))\n",
    "#    print('After OverSampling, the shape of train_y: {} \\n'.format(train_y.shape))\n",
    "\n",
    "#    for eachClass in classes:\n",
    "#        print(\"After OverSampling, counts of label \" + str(eachClass) + \": {}\".format(sum(train_y==eachClass)))  \n",
    "    \n",
    "    num_round = 20000 \n",
    "    train_data = pd.DataFrame(train_x)\n",
    "    train_y = pd.DataFrame(train_y)\n",
    "    train = lgb.Dataset(train_x, label=train_y)#, categorical_feature=categorical_feats)\n",
    "    val = lgb.Dataset(val_x, label=val_y)#, categorical_feature=categorical_feats)\n",
    "    \n",
    "    # Create parameters to search\n",
    "    fit_params={\"early_stopping_rounds\":100, \n",
    "                \"eval_metric\" : 'auc', \n",
    "                'eval_set' : [(val_x,val_y)],\n",
    "                'eval_names': ['valid'],\n",
    "                #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n",
    "                'verbose': 100}\n",
    "\n",
    "    clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=-1, device = 'gpu', n_estimators=5000)\n",
    "    gs = RandomizedSearchCV(\n",
    "        estimator=clf, param_distributions=param_test, \n",
    "        n_iter=num_round,\n",
    "        scoring='roc_auc',\n",
    "        cv=3,\n",
    "        refit=True,\n",
    "        random_state=314,\n",
    "        verbose=True)\n",
    "    \n",
    "    gs.fit(train_data, train_y, **fit_params)\n",
    "    oof[val_idx] = gs.predict(df_train.iloc[val_idx][ae_columns], num_iteration=clf.best_iteration)\n",
    "\n",
    "    \n",
    "    train_prediction = gs.predict(df_train[ae_columns] / folds.n_splits)\n",
    "    predictions += gs.predict(df_test[ae_columns] / folds.n_splits)\n",
    "    print(\"BEST PARAMETERS: \" + str(gs.best_params_))\n",
    "    print(\"BEST CV SCORE: \" + str(gs.best_score_))\n",
    "\n",
    "np.sqrt(mean_squared_error(oof, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\n",
    "sub_df[\"target\"] = predictions\n",
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_original.to_csv(\"df_original.csv\", index=False)\n",
    "df_train.to_csv(\"auto_model_reconstructions/train_auto_0.csv\", index=False)\n",
    "df_test.to_csv(\"auto_model_reconstructions/test_auto_0.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
