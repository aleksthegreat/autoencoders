{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wopr/.local/lib/python3.6/site-packages/tensorflow/python/ops/distributions/distribution.py:265: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /home/wopr/.local/lib/python3.6/site-packages/tensorflow/python/ops/distributions/bernoulli.py:169: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "from scipy.stats import norm, rankdata\n",
    "\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add,PReLU, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Reshape, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import tensorflow as tf\n",
    "import horovod.keras as hvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horovod: initialize Horovod.\n",
    "hvd.init()\n",
    "\n",
    "# Horovod: pin GPU to be used to process local rank (one GPU per process)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 78.01 MB\n",
      "Decreased by 74.7%\n",
      "Memory usage after optimization is: 77.82 MB\n",
      "Decreased by 74.6%\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(pd.read_csv('../input/train.csv'))\n",
    "test = reduce_mem_usage(pd.read_csv('../input/test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f for f in train if f not in ['ID_code','target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = pd.concat([train, test],axis=0,sort=False)\n",
    "df = df_original[features]\n",
    "target = df_original['target'].values\n",
    "id = df_original['ID_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for feature in features:\n",
    "#    df['mean_'+feature] = (train[feature].mean()-train[feature])\n",
    "#    df['z_'+feature] = (train[feature] - train[feature].mean())/train[feature].std(ddof=0)\n",
    "#    df['sq_'+feature] = (train[feature])**2\n",
    "#    df['sqrt_'+feature] = np.abs(train[feature])**(1/2)\n",
    "#    df['cp_'+feature] = pd.DataFrame(rankdata(train[feature]))\n",
    "#    df['cnp_'+feature] = pd.DataFrame((norm.cdf(train[feature])))\n",
    "#df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wopr/.local/lib/python3.6/site-packages/pandas/core/indexing.py:630: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "for df in [df]:\n",
    "#####Handling Missing Values#####     \n",
    "    for i in range(len(df.columns)):\n",
    "        df.iloc[:,i] = (df.iloc[:,i]).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wopr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/home/wopr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 155.64 MB\n",
      "Decreased by 74.6%\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import erfinv\n",
    "trafo_columns = [c for c in df.columns if len(df[c].unique()) != 2]\n",
    "for col in trafo_columns:\n",
    "    values = sorted(set(df[col]))\n",
    "    # Because erfinv(1) is inf, we shrink the range into (-0.9, 0.9)\n",
    "    f = pd.Series(np.linspace(-0.9, 0.9, len(values)), index=values)\n",
    "    f = np.sqrt(2) * erfinv(f)\n",
    "    f -= f.mean()\n",
    "    df[col] = df[col].map(f)\n",
    "\n",
    "df = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import PCA\n",
    "#pca = PCA(n_components=200)\n",
    "#pca.fit(df[trafo_columns])\n",
    "#df = pca.transform(df[trafo_columns])\n",
    "#df = pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wopr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.276123</td>\n",
       "      <td>-1.248047</td>\n",
       "      <td>0.492676</td>\n",
       "      <td>0.178345</td>\n",
       "      <td>0.301514</td>\n",
       "      <td>-1.103516</td>\n",
       "      <td>0.089050</td>\n",
       "      <td>0.470459</td>\n",
       "      <td>-1.241211</td>\n",
       "      <td>-0.489014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.264893</td>\n",
       "      <td>1.095703</td>\n",
       "      <td>0.690430</td>\n",
       "      <td>0.287354</td>\n",
       "      <td>-1.189453</td>\n",
       "      <td>1.255859</td>\n",
       "      <td>0.127563</td>\n",
       "      <td>-0.142212</td>\n",
       "      <td>-0.439697</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.544922</td>\n",
       "      <td>-1.052734</td>\n",
       "      <td>0.788086</td>\n",
       "      <td>0.232422</td>\n",
       "      <td>0.491943</td>\n",
       "      <td>1.179688</td>\n",
       "      <td>0.271484</td>\n",
       "      <td>0.289795</td>\n",
       "      <td>1.054688</td>\n",
       "      <td>0.565918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.720703</td>\n",
       "      <td>1.007812</td>\n",
       "      <td>1.376953</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>1.229492</td>\n",
       "      <td>1.270508</td>\n",
       "      <td>0.197388</td>\n",
       "      <td>0.609375</td>\n",
       "      <td>0.682129</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.245361</td>\n",
       "      <td>-0.890137</td>\n",
       "      <td>0.516602</td>\n",
       "      <td>0.739258</td>\n",
       "      <td>0.126343</td>\n",
       "      <td>-1.097656</td>\n",
       "      <td>0.821289</td>\n",
       "      <td>0.021896</td>\n",
       "      <td>-1.241211</td>\n",
       "      <td>-0.388916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.892090</td>\n",
       "      <td>0.833008</td>\n",
       "      <td>0.689453</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>1.481445</td>\n",
       "      <td>-1.262695</td>\n",
       "      <td>0.036865</td>\n",
       "      <td>0.174316</td>\n",
       "      <td>0.313965</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.497070</td>\n",
       "      <td>-0.824707</td>\n",
       "      <td>0.111084</td>\n",
       "      <td>0.583984</td>\n",
       "      <td>0.542480</td>\n",
       "      <td>-0.621582</td>\n",
       "      <td>0.354492</td>\n",
       "      <td>0.070740</td>\n",
       "      <td>-1.325195</td>\n",
       "      <td>0.606445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355225</td>\n",
       "      <td>0.544922</td>\n",
       "      <td>0.640137</td>\n",
       "      <td>0.805664</td>\n",
       "      <td>-0.897949</td>\n",
       "      <td>-0.942871</td>\n",
       "      <td>0.707520</td>\n",
       "      <td>0.571777</td>\n",
       "      <td>-0.994141</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.368164</td>\n",
       "      <td>-0.703613</td>\n",
       "      <td>0.632324</td>\n",
       "      <td>0.469238</td>\n",
       "      <td>0.472900</td>\n",
       "      <td>0.818848</td>\n",
       "      <td>0.391846</td>\n",
       "      <td>0.526855</td>\n",
       "      <td>1.381836</td>\n",
       "      <td>0.387207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.871094</td>\n",
       "      <td>-0.407227</td>\n",
       "      <td>1.296875</td>\n",
       "      <td>-0.517090</td>\n",
       "      <td>-0.965820</td>\n",
       "      <td>0.975586</td>\n",
       "      <td>0.424805</td>\n",
       "      <td>0.574707</td>\n",
       "      <td>-0.988770</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var_0     var_1     var_2     var_3     var_4     var_5     var_6  \\\n",
       "0  0.276123 -1.248047  0.492676  0.178345  0.301514 -1.103516  0.089050   \n",
       "1  0.544922 -1.052734  0.788086  0.232422  0.491943  1.179688  0.271484   \n",
       "2  0.245361 -0.890137  0.516602  0.739258  0.126343 -1.097656  0.821289   \n",
       "3  0.497070 -0.824707  0.111084  0.583984  0.542480 -0.621582  0.354492   \n",
       "4  0.368164 -0.703613  0.632324  0.469238  0.472900  0.818848  0.391846   \n",
       "\n",
       "      var_7     var_8     var_9   ...     var_191   var_192   var_193  \\\n",
       "0  0.470459 -1.241211 -0.489014   ...    0.264893  1.095703  0.690430   \n",
       "1  0.289795  1.054688  0.565918   ...    0.720703  1.007812  1.376953   \n",
       "2  0.021896 -1.241211 -0.388916   ...    0.892090  0.833008  0.689453   \n",
       "3  0.070740 -1.325195  0.606445   ...    0.355225  0.544922  0.640137   \n",
       "4  0.526855  1.381836  0.387207   ...    0.871094 -0.407227  1.296875   \n",
       "\n",
       "    var_194   var_195   var_196   var_197   var_198   var_199  target  \n",
       "0  0.287354 -1.189453  1.255859  0.127563 -0.142212 -0.439697     0.0  \n",
       "1 -0.075195  1.229492  1.270508  0.197388  0.609375  0.682129     0.0  \n",
       "2  0.625488  1.481445 -1.262695  0.036865  0.174316  0.313965     0.0  \n",
       "3  0.805664 -0.897949 -0.942871  0.707520  0.571777 -0.994141     0.0  \n",
       "4 -0.517090 -0.965820  0.975586  0.424805  0.574707 -0.988770     0.0  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'] = df_original.target.values\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = df[df['target'].notnull()]\n",
    "#target = train['target']\n",
    "#test = df[df['target'].isnull()]\n",
    "#trafo_columns = [c for c in train.columns if c not in ['target']]\n",
    "#train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafo_columns = [c for c in df.columns if c not in ['target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1500)              301500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1500)              6000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               300200    \n",
      "=================================================================\n",
      "Total params: 5,110,700\n",
      "Trainable params: 5,107,700\n",
      "Non-trainable params: 3,000\n",
      "_________________________________________________________________\n",
      "Train on 300000 samples, validate on 100000 samples\n",
      "Epoch 1/200\n",
      "300000/300000 [==============================] - 26s 87us/step - loss: 3.9700 - acc: 0.0135 - val_loss: 3.0568 - val_acc: 0.0163\n",
      "Epoch 2/200\n",
      "300000/300000 [==============================] - 25s 84us/step - loss: 2.3816 - acc: 0.0185 - val_loss: 1.8288 - val_acc: 0.0222\n",
      "Epoch 3/200\n",
      "300000/300000 [==============================] - 25s 84us/step - loss: 1.4288 - acc: 0.0298 - val_loss: 1.0736 - val_acc: 0.0416\n",
      "Epoch 4/200\n",
      "300000/300000 [==============================] - 25s 83us/step - loss: 0.7581 - acc: 0.0618 - val_loss: 0.4782 - val_acc: 0.0782\n",
      "Epoch 5/200\n",
      "300000/300000 [==============================] - 25s 83us/step - loss: 0.3043 - acc: 0.0907 - val_loss: 0.2207 - val_acc: 0.0908\n",
      "\n",
      "Epoch 5: finished gradual learning rate warmup to 0.01.\n",
      "Epoch 6/200\n",
      "300000/300000 [==============================] - 17s 58us/step - loss: 0.2073 - acc: 0.0955 - val_loss: 0.1968 - val_acc: 0.1003\n",
      "Epoch 7/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1886 - acc: 0.1095 - val_loss: 0.1805 - val_acc: 0.1106\n",
      "Epoch 8/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1723 - acc: 0.1263 - val_loss: 0.1643 - val_acc: 0.1368\n",
      "Epoch 9/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1567 - acc: 0.1434 - val_loss: 0.1490 - val_acc: 0.1435\n",
      "Epoch 10/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1419 - acc: 0.1653 - val_loss: 0.1357 - val_acc: 0.1585\n",
      "Epoch 11/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1290 - acc: 0.1896 - val_loss: 0.1236 - val_acc: 0.1906\n",
      "Epoch 12/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.1190 - acc: 0.2166 - val_loss: 0.1153 - val_acc: 0.2279\n",
      "Epoch 13/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.1116 - acc: 0.2422 - val_loss: 0.1091 - val_acc: 0.2254\n",
      "Epoch 14/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1063 - acc: 0.2648 - val_loss: 0.1047 - val_acc: 0.2454\n",
      "Epoch 15/200\n",
      "300000/300000 [==============================] - 17s 58us/step - loss: 0.1023 - acc: 0.2841 - val_loss: 0.1010 - val_acc: 0.2482\n",
      "Epoch 16/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0993 - acc: 0.3010 - val_loss: 0.0985 - val_acc: 0.2283\n",
      "Epoch 17/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0969 - acc: 0.3154 - val_loss: 0.0957 - val_acc: 0.2590\n",
      "Epoch 18/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0949 - acc: 0.3273 - val_loss: 0.0952 - val_acc: 0.2543\n",
      "Epoch 19/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0933 - acc: 0.3387 - val_loss: 0.0927 - val_acc: 0.2392\n",
      "Epoch 20/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0919 - acc: 0.3471 - val_loss: 0.0917 - val_acc: 0.2947\n",
      "Epoch 21/200\n",
      "300000/300000 [==============================] - 17s 58us/step - loss: 0.0907 - acc: 0.3543 - val_loss: 0.0909 - val_acc: 0.2867\n",
      "Epoch 22/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0896 - acc: 0.3608 - val_loss: 0.0890 - val_acc: 0.2983\n",
      "Epoch 23/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0886 - acc: 0.3670 - val_loss: 0.0887 - val_acc: 0.3047\n",
      "Epoch 24/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0878 - acc: 0.3726 - val_loss: 0.0889 - val_acc: 0.2630\n",
      "Epoch 25/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0870 - acc: 0.3770 - val_loss: 0.0882 - val_acc: 0.2228\n",
      "Epoch 26/200\n",
      "300000/300000 [==============================] - 17s 58us/step - loss: 0.0863 - acc: 0.3795 - val_loss: 0.0865 - val_acc: 0.2438\n",
      "Epoch 27/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0857 - acc: 0.3828 - val_loss: 0.0856 - val_acc: 0.2498\n",
      "Epoch 28/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0851 - acc: 0.3860 - val_loss: 0.0873 - val_acc: 0.2471\n",
      "Epoch 29/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0845 - acc: 0.3887 - val_loss: 0.0856 - val_acc: 0.2179\n",
      "Epoch 30/200\n",
      "300000/300000 [==============================] - 17s 58us/step - loss: 0.0840 - acc: 0.3915 - val_loss: 0.0863 - val_acc: 0.2355\n",
      "Epoch 31/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0836 - acc: 0.3916 - val_loss: 0.0854 - val_acc: 0.2335\n",
      "Epoch 32/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0832 - acc: 0.3945 - val_loss: 0.0849 - val_acc: 0.2751\n",
      "Epoch 33/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0828 - acc: 0.3960 - val_loss: 0.0852 - val_acc: 0.2394\n",
      "Epoch 34/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0824 - acc: 0.3971 - val_loss: 0.0838 - val_acc: 0.2146\n",
      "Epoch 35/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0821 - acc: 0.3958 - val_loss: 0.0849 - val_acc: 0.2176\n",
      "Epoch 00035: early stopping\n",
      "100000/100000 [==============================] - 4s 39us/step\n",
      "400000/400000 [==============================] - 15s 38us/step\n",
      "fold 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1500)              301500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1500)              6000      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 200)               300200    \n",
      "=================================================================\n",
      "Total params: 5,110,700\n",
      "Trainable params: 5,107,700\n",
      "Non-trainable params: 3,000\n",
      "_________________________________________________________________\n",
      "Train on 300000 samples, validate on 100000 samples\n",
      "Epoch 1/200\n",
      "300000/300000 [==============================] - 27s 89us/step - loss: 3.9573 - acc: 0.0144 - val_loss: 3.0399 - val_acc: 0.0181\n",
      "Epoch 2/200\n",
      "300000/300000 [==============================] - 26s 88us/step - loss: 2.3628 - acc: 0.0210 - val_loss: 1.8272 - val_acc: 0.0258\n",
      "Epoch 3/200\n",
      "300000/300000 [==============================] - 26s 86us/step - loss: 1.4273 - acc: 0.0284 - val_loss: 1.0839 - val_acc: 0.0326\n",
      "Epoch 4/200\n",
      "300000/300000 [==============================] - 26s 88us/step - loss: 0.7669 - acc: 0.0386 - val_loss: 0.4862 - val_acc: 0.0526\n",
      "Epoch 5/200\n",
      "300000/300000 [==============================] - 26s 88us/step - loss: 0.3074 - acc: 0.0825 - val_loss: 0.2193 - val_acc: 0.0925\n",
      "\n",
      "Epoch 5: finished gradual learning rate warmup to 0.01.\n",
      "Epoch 6/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.2060 - acc: 0.0970 - val_loss: 0.1951 - val_acc: 0.1021\n",
      "Epoch 7/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1871 - acc: 0.1111 - val_loss: 0.1779 - val_acc: 0.1121\n",
      "Epoch 8/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1705 - acc: 0.1271 - val_loss: 0.1615 - val_acc: 0.1342\n",
      "Epoch 9/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.1546 - acc: 0.1453 - val_loss: 0.1468 - val_acc: 0.1584\n",
      "Epoch 10/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1396 - acc: 0.1664 - val_loss: 0.1323 - val_acc: 0.1660\n",
      "Epoch 11/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.1270 - acc: 0.1916 - val_loss: 0.1222 - val_acc: 0.1976\n",
      "Epoch 12/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1174 - acc: 0.2178 - val_loss: 0.1133 - val_acc: 0.2197\n",
      "Epoch 13/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1104 - acc: 0.2427 - val_loss: 0.1079 - val_acc: 0.2617\n",
      "Epoch 14/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1054 - acc: 0.2643 - val_loss: 0.1048 - val_acc: 0.2426\n",
      "Epoch 15/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.1016 - acc: 0.2836 - val_loss: 0.1001 - val_acc: 0.2521\n",
      "Epoch 16/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0987 - acc: 0.3002 - val_loss: 0.0968 - val_acc: 0.2595\n",
      "Epoch 17/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0964 - acc: 0.3164 - val_loss: 0.0951 - val_acc: 0.2566\n",
      "Epoch 18/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0945 - acc: 0.3263 - val_loss: 0.0941 - val_acc: 0.2372\n",
      "Epoch 19/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0929 - acc: 0.3358 - val_loss: 0.0912 - val_acc: 0.2964\n",
      "Epoch 20/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0916 - acc: 0.3434 - val_loss: 0.0916 - val_acc: 0.2890\n",
      "Epoch 21/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0903 - acc: 0.3525 - val_loss: 0.0893 - val_acc: 0.3110\n",
      "Epoch 22/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0893 - acc: 0.3589 - val_loss: 0.0883 - val_acc: 0.2997\n",
      "Epoch 23/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0884 - acc: 0.3646 - val_loss: 0.0881 - val_acc: 0.2628\n",
      "Epoch 24/200\n",
      "300000/300000 [==============================] - 17s 58us/step - loss: 0.0875 - acc: 0.3704 - val_loss: 0.0882 - val_acc: 0.2474\n",
      "Epoch 25/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0867 - acc: 0.3748 - val_loss: 0.0870 - val_acc: 0.2627\n",
      "Epoch 26/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0860 - acc: 0.3797 - val_loss: 0.0866 - val_acc: 0.2860\n",
      "Epoch 27/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0854 - acc: 0.3827 - val_loss: 0.0851 - val_acc: 0.2977\n",
      "Epoch 28/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0848 - acc: 0.3837 - val_loss: 0.0860 - val_acc: 0.2576\n",
      "Epoch 29/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0842 - acc: 0.3866 - val_loss: 0.0847 - val_acc: 0.2715\n",
      "Epoch 30/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0838 - acc: 0.3897 - val_loss: 0.0868 - val_acc: 0.1920\n",
      "Epoch 31/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0833 - acc: 0.3918 - val_loss: 0.0836 - val_acc: 0.1925\n",
      "Epoch 32/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0829 - acc: 0.3946 - val_loss: 0.0838 - val_acc: 0.2984\n",
      "Epoch 33/200\n",
      "300000/300000 [==============================] - 17s 58us/step - loss: 0.0825 - acc: 0.3952 - val_loss: 0.0854 - val_acc: 0.2848\n",
      "Epoch 00033: early stopping\n",
      "100000/100000 [==============================] - 4s 40us/step\n",
      "400000/400000 [==============================] - 15s 39us/step\n",
      "fold 2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1500)              301500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 1500)              6000      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 200)               300200    \n",
      "=================================================================\n",
      "Total params: 5,110,700\n",
      "Trainable params: 5,107,700\n",
      "Non-trainable params: 3,000\n",
      "_________________________________________________________________\n",
      "Train on 300000 samples, validate on 100000 samples\n",
      "Epoch 1/200\n",
      "300000/300000 [==============================] - 27s 88us/step - loss: 3.9474 - acc: 0.0137 - val_loss: 3.0321 - val_acc: 0.0164\n",
      "Epoch 2/200\n",
      "300000/300000 [==============================] - 26s 86us/step - loss: 2.3485 - acc: 0.0195 - val_loss: 1.8280 - val_acc: 0.0234\n",
      "Epoch 3/200\n",
      "300000/300000 [==============================] - 26s 87us/step - loss: 1.4211 - acc: 0.0339 - val_loss: 1.0897 - val_acc: 0.0497\n",
      "Epoch 4/200\n",
      "300000/300000 [==============================] - 25s 85us/step - loss: 0.7693 - acc: 0.0639 - val_loss: 0.4952 - val_acc: 0.0842\n",
      "Epoch 5/200\n",
      "300000/300000 [==============================] - 26s 85us/step - loss: 0.3117 - acc: 0.0909 - val_loss: 0.2206 - val_acc: 0.0957\n",
      "\n",
      "Epoch 5: finished gradual learning rate warmup to 0.01.\n",
      "Epoch 6/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.2067 - acc: 0.0956 - val_loss: 0.1952 - val_acc: 0.1061\n",
      "Epoch 7/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1880 - acc: 0.1085 - val_loss: 0.1788 - val_acc: 0.1174\n",
      "Epoch 8/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.1718 - acc: 0.1244 - val_loss: 0.1635 - val_acc: 0.1336\n",
      "Epoch 9/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.1562 - acc: 0.1413 - val_loss: 0.1481 - val_acc: 0.1537\n",
      "Epoch 10/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1414 - acc: 0.1628 - val_loss: 0.1348 - val_acc: 0.1676\n",
      "Epoch 11/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1286 - acc: 0.1868 - val_loss: 0.1224 - val_acc: 0.1816\n",
      "Epoch 12/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1186 - acc: 0.2119 - val_loss: 0.1152 - val_acc: 0.2037\n",
      "Epoch 13/200\n",
      "300000/300000 [==============================] - 17s 58us/step - loss: 0.1113 - acc: 0.2363 - val_loss: 0.1087 - val_acc: 0.2309\n",
      "Epoch 14/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.1060 - acc: 0.2598 - val_loss: 0.1044 - val_acc: 0.2299\n",
      "Epoch 15/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.1021 - acc: 0.2789 - val_loss: 0.1008 - val_acc: 0.2678\n",
      "Epoch 16/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0991 - acc: 0.2963 - val_loss: 0.0983 - val_acc: 0.2811\n",
      "Epoch 17/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0968 - acc: 0.3116 - val_loss: 0.0961 - val_acc: 0.2500\n",
      "Epoch 18/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0948 - acc: 0.3225 - val_loss: 0.0943 - val_acc: 0.2597\n",
      "Epoch 19/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0932 - acc: 0.3329 - val_loss: 0.0927 - val_acc: 0.2405\n",
      "Epoch 20/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0918 - acc: 0.3419 - val_loss: 0.0922 - val_acc: 0.2476\n",
      "Epoch 21/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0906 - acc: 0.3505 - val_loss: 0.0901 - val_acc: 0.2838\n",
      "Epoch 22/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0895 - acc: 0.3572 - val_loss: 0.0889 - val_acc: 0.2520\n",
      "Epoch 23/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0885 - acc: 0.3638 - val_loss: 0.0889 - val_acc: 0.2712\n",
      "Epoch 24/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0877 - acc: 0.3669 - val_loss: 0.0882 - val_acc: 0.3148\n",
      "Epoch 25/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0869 - acc: 0.3716 - val_loss: 0.0876 - val_acc: 0.2947\n",
      "Epoch 26/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0862 - acc: 0.3768 - val_loss: 0.0862 - val_acc: 0.2191\n",
      "Epoch 27/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0855 - acc: 0.3811 - val_loss: 0.0857 - val_acc: 0.2818\n",
      "Epoch 28/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0849 - acc: 0.3831 - val_loss: 0.0862 - val_acc: 0.2287\n",
      "Epoch 29/200\n",
      "300000/300000 [==============================] - 17s 58us/step - loss: 0.0844 - acc: 0.3866 - val_loss: 0.0844 - val_acc: 0.2581\n",
      "Epoch 30/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0839 - acc: 0.3887 - val_loss: 0.0856 - val_acc: 0.2217\n",
      "Epoch 31/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0834 - acc: 0.3905 - val_loss: 0.0861 - val_acc: 0.2423\n",
      "Epoch 32/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0830 - acc: 0.3918 - val_loss: 0.0854 - val_acc: 0.3078\n",
      "Epoch 33/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0826 - acc: 0.3934 - val_loss: 0.0842 - val_acc: 0.2375\n",
      "Epoch 34/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0822 - acc: 0.3949 - val_loss: 0.0866 - val_acc: 0.2000\n",
      "Epoch 35/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0819 - acc: 0.3945 - val_loss: 0.0843 - val_acc: 0.2488\n",
      "Epoch 36/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0816 - acc: 0.3974 - val_loss: 0.0832 - val_acc: 0.2291\n",
      "Epoch 00036: early stopping\n",
      "100000/100000 [==============================] - 4s 40us/step\n",
      "400000/400000 [==============================] - 15s 39us/step\n",
      "fold 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1500)              301500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1500)              6000      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1500)              2251500   \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 200)               300200    \n",
      "=================================================================\n",
      "Total params: 5,110,700\n",
      "Trainable params: 5,107,700\n",
      "Non-trainable params: 3,000\n",
      "_________________________________________________________________\n",
      "Train on 300000 samples, validate on 100000 samples\n",
      "Epoch 1/200\n",
      "300000/300000 [==============================] - 27s 91us/step - loss: 3.9682 - acc: 0.0134 - val_loss: 3.0435 - val_acc: 0.0161\n",
      "Epoch 2/200\n",
      "300000/300000 [==============================] - 27s 89us/step - loss: 2.3663 - acc: 0.0165 - val_loss: 1.8314 - val_acc: 0.0176\n",
      "Epoch 3/200\n",
      "300000/300000 [==============================] - 26s 88us/step - loss: 1.4297 - acc: 0.0232 - val_loss: 1.0891 - val_acc: 0.0311\n",
      "Epoch 4/200\n",
      "300000/300000 [==============================] - 27s 89us/step - loss: 0.7705 - acc: 0.0535 - val_loss: 0.4925 - val_acc: 0.0729\n",
      "Epoch 5/200\n",
      "300000/300000 [==============================] - 27s 88us/step - loss: 0.3105 - acc: 0.0915 - val_loss: 0.2206 - val_acc: 0.0904\n",
      "\n",
      "Epoch 5: finished gradual learning rate warmup to 0.01.\n",
      "Epoch 6/200\n",
      "300000/300000 [==============================] - 17s 58us/step - loss: 0.2069 - acc: 0.0980 - val_loss: 0.1952 - val_acc: 0.0992\n",
      "Epoch 7/200\n",
      "300000/300000 [==============================] - 17s 58us/step - loss: 0.1878 - acc: 0.1118 - val_loss: 0.1781 - val_acc: 0.1129\n",
      "Epoch 8/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1712 - acc: 0.1286 - val_loss: 0.1618 - val_acc: 0.1341\n",
      "Epoch 9/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.1552 - acc: 0.1463 - val_loss: 0.1468 - val_acc: 0.1515\n",
      "Epoch 10/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.1403 - acc: 0.1693 - val_loss: 0.1326 - val_acc: 0.1620\n",
      "Epoch 11/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.1276 - acc: 0.1946 - val_loss: 0.1221 - val_acc: 0.1859\n",
      "Epoch 12/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.1179 - acc: 0.2207 - val_loss: 0.1136 - val_acc: 0.1966\n",
      "Epoch 13/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.1108 - acc: 0.2456 - val_loss: 0.1081 - val_acc: 0.2301\n",
      "Epoch 14/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.1057 - acc: 0.2684 - val_loss: 0.1033 - val_acc: 0.2493\n",
      "Epoch 15/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.1018 - acc: 0.2875 - val_loss: 0.1008 - val_acc: 0.2504\n",
      "Epoch 16/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0989 - acc: 0.3044 - val_loss: 0.0971 - val_acc: 0.2692\n",
      "Epoch 17/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0965 - acc: 0.3170 - val_loss: 0.0959 - val_acc: 0.3091\n",
      "Epoch 18/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0946 - acc: 0.3286 - val_loss: 0.0938 - val_acc: 0.2776\n",
      "Epoch 19/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0930 - acc: 0.3378 - val_loss: 0.0925 - val_acc: 0.2731\n",
      "Epoch 20/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0916 - acc: 0.3479 - val_loss: 0.0913 - val_acc: 0.2158\n",
      "Epoch 21/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0903 - acc: 0.3548 - val_loss: 0.0898 - val_acc: 0.2419\n",
      "Epoch 22/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0893 - acc: 0.3625 - val_loss: 0.0890 - val_acc: 0.2519\n",
      "Epoch 23/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0883 - acc: 0.3676 - val_loss: 0.0877 - val_acc: 0.2804\n",
      "Epoch 24/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0875 - acc: 0.3719 - val_loss: 0.0878 - val_acc: 0.3113\n",
      "Epoch 25/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0867 - acc: 0.3767 - val_loss: 0.0866 - val_acc: 0.2741\n",
      "Epoch 26/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0860 - acc: 0.3794 - val_loss: 0.0867 - val_acc: 0.3428\n",
      "Epoch 27/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0853 - acc: 0.3830 - val_loss: 0.0856 - val_acc: 0.2659\n",
      "Epoch 28/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0847 - acc: 0.3864 - val_loss: 0.0847 - val_acc: 0.2525\n",
      "Epoch 29/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0841 - acc: 0.3891 - val_loss: 0.0859 - val_acc: 0.2243\n",
      "Epoch 30/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0836 - acc: 0.3894 - val_loss: 0.0846 - val_acc: 0.2595\n",
      "Epoch 31/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0832 - acc: 0.3919 - val_loss: 0.0830 - val_acc: 0.1469\n",
      "Epoch 32/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0827 - acc: 0.3940 - val_loss: 0.0830 - val_acc: 0.2314\n",
      "Epoch 33/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0823 - acc: 0.3963 - val_loss: 0.0848 - val_acc: 0.1972\n",
      "Epoch 34/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0819 - acc: 0.3974 - val_loss: 0.0853 - val_acc: 0.1601\n",
      "Epoch 35/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0816 - acc: 0.3974 - val_loss: 0.0849 - val_acc: 0.1689\n",
      "Epoch 36/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0813 - acc: 0.3974 - val_loss: 0.0833 - val_acc: 0.2143\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "Epoch 37/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0749 - acc: 0.4176 - val_loss: 0.0737 - val_acc: 0.3937\n",
      "Epoch 38/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0746 - acc: 0.4182 - val_loss: 0.0735 - val_acc: 0.3765\n",
      "Epoch 39/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0744 - acc: 0.4174 - val_loss: 0.0734 - val_acc: 0.3736\n",
      "Epoch 40/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0742 - acc: 0.4180 - val_loss: 0.0733 - val_acc: 0.3605\n",
      "Epoch 41/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0742 - acc: 0.4155 - val_loss: 0.0731 - val_acc: 0.3865\n",
      "Epoch 42/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0741 - acc: 0.4146 - val_loss: 0.0731 - val_acc: 0.3850\n",
      "Epoch 43/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0740 - acc: 0.4157 - val_loss: 0.0730 - val_acc: 0.4016\n",
      "Epoch 44/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0739 - acc: 0.4151 - val_loss: 0.0729 - val_acc: 0.3809\n",
      "Epoch 45/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0739 - acc: 0.4156 - val_loss: 0.0729 - val_acc: 0.3705\n",
      "Epoch 46/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0738 - acc: 0.4158 - val_loss: 0.0728 - val_acc: 0.3809\n",
      "Epoch 47/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0737 - acc: 0.4147 - val_loss: 0.0728 - val_acc: 0.3931\n",
      "Epoch 48/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0737 - acc: 0.4150 - val_loss: 0.0727 - val_acc: 0.3838\n",
      "Epoch 49/200\n",
      "300000/300000 [==============================] - 18s 59us/step - loss: 0.0736 - acc: 0.4169 - val_loss: 0.0726 - val_acc: 0.3881\n",
      "Epoch 50/200\n",
      "300000/300000 [==============================] - 19s 62us/step - loss: 0.0736 - acc: 0.4160 - val_loss: 0.0726 - val_acc: 0.3810\n",
      "Epoch 51/200\n",
      "300000/300000 [==============================] - 18s 61us/step - loss: 0.0735 - acc: 0.4152 - val_loss: 0.0725 - val_acc: 0.3920\n",
      "Epoch 52/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0734 - acc: 0.4163 - val_loss: 0.0724 - val_acc: 0.3903\n",
      "Epoch 53/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0734 - acc: 0.4168 - val_loss: 0.0724 - val_acc: 0.3871\n",
      "Epoch 54/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0733 - acc: 0.4172 - val_loss: 0.0723 - val_acc: 0.3927\n",
      "Epoch 55/200\n",
      "300000/300000 [==============================] - 18s 60us/step - loss: 0.0733 - acc: 0.4154 - val_loss: 0.0722 - val_acc: 0.3880\n",
      "Epoch 00055: early stopping\n",
      "100000/100000 [==============================] - 4s 41us/step\n",
      "400000/400000 [==============================] - 16s 40us/step\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.activations import elu\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras.objectives import binary_crossentropy\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from imblearn.keras import balanced_batch_generator\n",
    "from imblearn.under_sampling import NearMiss, RandomUnderSampler, CondensedNearestNeighbour, AllKNN, InstanceHardnessThreshold\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.utils import multi_gpu_model\n",
    "import math\n",
    "\n",
    "learning_rate = 0.0001\n",
    "mom = 0.15\n",
    "dcy = 0.996\n",
    "nb_folds = 4\n",
    "nb_epoch = int(math.ceil(200.0 / hvd.size()))\n",
    "batch_size = 128\n",
    "encoding_dim =1500\n",
    "hidden_dim = int(encoding_dim) #i.e. 7\n",
    "sgd = SGD(lr=learning_rate, momentum=mom, decay=dcy)\n",
    "#folds = StratifiedKFold(n_splits=nb_folds, shuffle=True, random_state=420)\n",
    "folds = KFold(n_splits = nb_folds, random_state = 338, shuffle = True)\n",
    "auto = np.zeros(df[trafo_columns].shape)\n",
    "layer_output = np.zeros((len(df), encoding_dim)) # change when nn shape changes\n",
    "#layer_output = np.zeros(df[trafo_columns].shape)\n",
    "#train_auto = np.zeros(train[trafo_columns].shape)\n",
    "#test_auto = np.zeros(test[trafo_columns].shape)\n",
    "predictions = np.zeros(len(df))\n",
    "#label_cols = [\"target\"]\n",
    "#y_split = train[label_cols].values\n",
    "\n",
    "# Horovod: adjust learning rate based on number of GPUs.\n",
    "opt = keras.optimizers.SGD(lr=(learning_rate * hvd.size()), decay=dcy, momentum=mom, nesterov=True)\n",
    "# Horovod: add Horovod Distributed Optimizer.\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "cp = ModelCheckpoint(filepath=\"autoencoder_0.h5\",\n",
    "                               save_best_only=True,\n",
    "                               verbose=0)\n",
    "\n",
    "tb = TensorBoard(log_dir='./logs',\n",
    "                histogram_freq=0,\n",
    "                write_graph=True,\n",
    "                write_images=True)\n",
    "\n",
    "es= EarlyStopping(monitor='val_acc',\n",
    "                  min_delta=0,\n",
    "                  patience=10,\n",
    "                  verbose=1, mode='min')\n",
    "\n",
    "\n",
    "    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "    # This is necessary to ensure consistent initialization of all workers when\n",
    "    # training is started with random weights or restored from a checkpoint.\n",
    "bgvc= hvd.callbacks.BroadcastGlobalVariablesCallback(0)\n",
    "\n",
    "    # Horovod: average metrics among workers at the end of every epoch.\n",
    "    #\n",
    "    # Note: This callback must be in the list before the ReduceLROnPlateau,\n",
    "    # TensorBoard or other metrics-based callbacks.\n",
    "mac= hvd.callbacks.MetricAverageCallback()\n",
    "\n",
    "    # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n",
    "    # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n",
    "    # the first five epochs. See https://arxiv.org/abs/1706.02677 for details.\n",
    "lrwc = hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=5, verbose=1)\n",
    "\n",
    "    # Reduce the learning rate if training plateaues.\n",
    "rlp = keras.callbacks.ReduceLROnPlateau(patience=5, verbose=1)\n",
    "\n",
    "\n",
    "#for fold_, (trn_idx, val_idx) in enumerate(folds.split(y_split[:,0], y_split[:,0])):\n",
    "#    print(\"fold {}\".format(fold_))\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "\n",
    "    trn_data = df[trafo_columns].iloc[trn_idx]\n",
    "    val_data = df[trafo_columns].iloc[val_idx]\n",
    "\n",
    "    def add_noise(series, noise_level):\n",
    "        return series * (1 + noise_level * np.random.randn(series.shape[1]))\n",
    "    \n",
    "    noisy_trn_data = add_noise(trn_data, 0.05)\n",
    "\n",
    "    input_dim = noisy_trn_data.shape[1] #num of columns, 30\n",
    "    input_layer = Input(shape=(input_dim, ))\n",
    "    encoder = Dense(encoding_dim, activation=\"tanh\", activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
    "    encoder = BatchNormalization()(encoder)\n",
    "    encoder = Dense(hidden_dim, activation=\"relu\")(encoder)\n",
    "    decoder = Dense(hidden_dim, activation='relu')(encoder)\n",
    "    decoder = Dense(input_dim, activation='tanh')(decoder)\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)    \n",
    "#    model = Model(inputs=input_layer, outputs=decoder)\n",
    "#   autoencoder = multi_gpu_model(model, gpus=2)\n",
    "    autoencoder.summary()\n",
    "    \n",
    "    autoencoder.compile(metrics=['accuracy'],\n",
    "                        loss='mean_squared_error',\n",
    "                        optimizer='sgd')\n",
    "\n",
    "    cp = ModelCheckpoint(filepath=\"autoencoder_fraud.h5\",\n",
    "                                   save_best_only=True,\n",
    "                                   verbose=0)\n",
    "\n",
    "    tb = TensorBoard(log_dir='./logs',\n",
    "                    histogram_freq=0,\n",
    "                    write_graph=True,\n",
    "                    write_images=True)\n",
    "    \n",
    "    es= EarlyStopping(monitor='val_acc',\n",
    "                  min_delta=0,\n",
    "                  patience=12,\n",
    "                  verbose=1, mode='auto')\n",
    "\n",
    "    history = autoencoder.fit(noisy_trn_data, trn_data,\n",
    "                        epochs=nb_epoch,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        validation_data=(val_data, val_data),\n",
    "                        verbose=1,\n",
    "                        callbacks=[cp, tb, es, bgvc, mac, lrwc, rlp]).history\n",
    "    \n",
    "    \n",
    "    auto[val_idx] += autoencoder.predict(df.iloc[val_idx][trafo_columns], verbose=1)\n",
    "    mse = autoencoder.predict(df[trafo_columns] / folds.n_splits, verbose=1)\n",
    "    predictions += np.mean(np.power(df[trafo_columns] - mse, 2), axis=1)\n",
    "    # we build a new model with the activations of the old model\n",
    "    # this model is truncated after the first layer\n",
    "    get_1st_layer_output = K.function([autoencoder.layers[0].input],\n",
    "                                  [autoencoder.layers[1].output])\n",
    "    layer_output[val_idx] += pd.DataFrame(np.concatenate(get_1st_layer_output([df.iloc[val_idx][trafo_columns]])))\n",
    "    \n",
    "auto_final = pd.DataFrame(auto / folds.n_splits)\n",
    "hidden = pd.DataFrame(layer_output / folds.n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>target</th>\n",
       "      <th>ID_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.095245</td>\n",
       "      <td>-0.232096</td>\n",
       "      <td>0.114008</td>\n",
       "      <td>0.052224</td>\n",
       "      <td>0.060247</td>\n",
       "      <td>-0.235883</td>\n",
       "      <td>0.008829</td>\n",
       "      <td>0.152451</td>\n",
       "      <td>-0.234009</td>\n",
       "      <td>-0.109070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219451</td>\n",
       "      <td>0.166259</td>\n",
       "      <td>0.077680</td>\n",
       "      <td>-0.242313</td>\n",
       "      <td>0.239918</td>\n",
       "      <td>0.042230</td>\n",
       "      <td>-0.002304</td>\n",
       "      <td>-0.158073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.144771</td>\n",
       "      <td>-0.237629</td>\n",
       "      <td>0.152369</td>\n",
       "      <td>0.055982</td>\n",
       "      <td>0.128035</td>\n",
       "      <td>0.232224</td>\n",
       "      <td>0.065603</td>\n",
       "      <td>0.065170</td>\n",
       "      <td>0.199946</td>\n",
       "      <td>0.129121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209524</td>\n",
       "      <td>0.240359</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.236910</td>\n",
       "      <td>0.240021</td>\n",
       "      <td>0.041866</td>\n",
       "      <td>0.077078</td>\n",
       "      <td>0.190503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.071671</td>\n",
       "      <td>-0.227840</td>\n",
       "      <td>0.108156</td>\n",
       "      <td>0.158549</td>\n",
       "      <td>0.049578</td>\n",
       "      <td>-0.226953</td>\n",
       "      <td>0.160241</td>\n",
       "      <td>0.010538</td>\n",
       "      <td>-0.239851</td>\n",
       "      <td>-0.109305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188004</td>\n",
       "      <td>0.208677</td>\n",
       "      <td>0.156755</td>\n",
       "      <td>0.241404</td>\n",
       "      <td>-0.221771</td>\n",
       "      <td>-0.000449</td>\n",
       "      <td>-0.004868</td>\n",
       "      <td>0.094834</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.133116</td>\n",
       "      <td>-0.193924</td>\n",
       "      <td>0.022620</td>\n",
       "      <td>0.147084</td>\n",
       "      <td>0.127124</td>\n",
       "      <td>-0.208932</td>\n",
       "      <td>0.078299</td>\n",
       "      <td>0.025473</td>\n",
       "      <td>-0.230423</td>\n",
       "      <td>0.139540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179277</td>\n",
       "      <td>0.190620</td>\n",
       "      <td>0.168366</td>\n",
       "      <td>-0.184700</td>\n",
       "      <td>-0.201374</td>\n",
       "      <td>0.159318</td>\n",
       "      <td>0.113921</td>\n",
       "      <td>-0.193304</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.115653</td>\n",
       "      <td>-0.181926</td>\n",
       "      <td>0.145287</td>\n",
       "      <td>0.120495</td>\n",
       "      <td>0.096480</td>\n",
       "      <td>0.167270</td>\n",
       "      <td>0.079684</td>\n",
       "      <td>0.161471</td>\n",
       "      <td>0.240113</td>\n",
       "      <td>0.104960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106068</td>\n",
       "      <td>0.227203</td>\n",
       "      <td>-0.122813</td>\n",
       "      <td>-0.231156</td>\n",
       "      <td>0.226698</td>\n",
       "      <td>0.109604</td>\n",
       "      <td>0.149273</td>\n",
       "      <td>-0.227088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>train_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.095245 -0.232096  0.114008  0.052224  0.060247 -0.235883  0.008829   \n",
       "1  0.144771 -0.237629  0.152369  0.055982  0.128035  0.232224  0.065603   \n",
       "2  0.071671 -0.227840  0.108156  0.158549  0.049578 -0.226953  0.160241   \n",
       "3  0.133116 -0.193924  0.022620  0.147084  0.127124 -0.208932  0.078299   \n",
       "4  0.115653 -0.181926  0.145287  0.120495  0.096480  0.167270  0.079684   \n",
       "\n",
       "          7         8         9   ...          192       193       194  \\\n",
       "0  0.152451 -0.234009 -0.109070   ...     0.219451  0.166259  0.077680   \n",
       "1  0.065170  0.199946  0.129121   ...     0.209524  0.240359  0.014283   \n",
       "2  0.010538 -0.239851 -0.109305   ...     0.188004  0.208677  0.156755   \n",
       "3  0.025473 -0.230423  0.139540   ...     0.179277  0.190620  0.168366   \n",
       "4  0.161471  0.240113  0.104960   ...    -0.106068  0.227203 -0.122813   \n",
       "\n",
       "        195       196       197       198       199  target  ID_code  \n",
       "0 -0.242313  0.239918  0.042230 -0.002304 -0.158073     0.0  train_0  \n",
       "1  0.236910  0.240021  0.041866  0.077078  0.190503     0.0  train_1  \n",
       "2  0.241404 -0.221771 -0.000449 -0.004868  0.094834     0.0  train_2  \n",
       "3 -0.184700 -0.201374  0.159318  0.113921 -0.193304     0.0  train_3  \n",
       "4 -0.231156  0.226698  0.109604  0.149273 -0.227088     0.0  train_4  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hidden\n",
    "hidden['target'] = target\n",
    "hidden['ID_code'] = id.values\n",
    "hidden.head(5)\n",
    "#final\n",
    "auto_final['target'] = target\n",
    "auto_final['ID_code'] = id.values\n",
    "auto_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable         Type         Data/Info\n",
      "---------------------------------------\n",
      "auto_final       DataFrame                   0         <...>00000 rows x 202 columns]\n",
      "df               DataFrame               var_0     var_<...>00000 rows x 201 columns]\n",
      "df_original      DataFrame                ID_code  targ<...>00000 rows x 202 columns]\n",
      "hidden           DataFrame                   0         <...>0000 rows x 1502 columns]\n",
      "noisy_trn_data   DataFrame               var_0     var_<...>00000 rows x 200 columns]\n",
      "test             DataFrame                ID_code      <...>00000 rows x 201 columns]\n",
      "train            DataFrame                 ID_code  tar<...>00000 rows x 202 columns]\n",
      "trn_data         DataFrame               var_0     var_<...>00000 rows x 200 columns]\n",
      "val_data         DataFrame               var_0     var_<...>00000 rows x 200 columns]\n"
     ]
    }
   ],
   "source": [
    "%whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_original, df, noisy_trn_data, test, train, trn_data, val_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable      Type         Data/Info\n",
      "------------------------------------\n",
      "auto_final    DataFrame                   0         <...>00000 rows x 202 columns]\n",
      "df_original   DataFrame                ID_code  targ<...>00000 rows x 202 columns]\n",
      "hidden        DataFrame                   0         <...>0000 rows x 1502 columns]\n"
     ]
    }
   ],
   "source": [
    "%whos DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 202)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hidden\n",
    "#ae_columns = [c for c in hidden.columns if c not in ['ID_code', 'target']]\n",
    "#df_train = hidden[hidden['target'].notnull()]\n",
    "#target = df_train['target']\n",
    "#df_test = hidden[hidden['target'].isnull()]\n",
    "#df_train.shape\n",
    "\n",
    "#final\n",
    "ae_columns = [c for c in auto_final.columns if c not in ['ID_code', 'target']]\n",
    "df_train = auto_final[auto_final['target'].notnull()]\n",
    "target = df_train['target']\n",
    "df_test = auto_final[auto_final['target'].isnull()]\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wopr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/wopr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/wopr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/wopr/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "df_train['ID_code'] = df_train['ID_code'].astype('category')\n",
    "df_test['ID_code'] = df_test['ID_code'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 96 candidates, totalling 384 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=6)]: Done   1 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=6)]: Done   6 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=6)]: Done  13 tasks      | elapsed:   20.2s\n",
      "[Parallel(n_jobs=6)]: Done  20 tasks      | elapsed:   26.3s\n",
      "[Parallel(n_jobs=6)]: Done  29 tasks      | elapsed:   36.1s\n",
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:   44.0s\n",
      "[Parallel(n_jobs=6)]: Done  49 tasks      | elapsed:   56.6s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "#%% Prepare data\n",
    "def prepLGB(data,\n",
    "            classCol='',\n",
    "            IDCol='',\n",
    "            fDrop=[]):\n",
    "\n",
    "        # Drop class column\n",
    "        if classCol != '':\n",
    "            labels = data[classCol]\n",
    "#            fDrop = fDrop + [classCol]\n",
    "        else:\n",
    "            labels = []\n",
    "\n",
    "        if IDCol != '':\n",
    "            IDs = data[IDCol]\n",
    "#            fDrop = fDrop + [IDCol]\n",
    "        else:\n",
    "            IDs = []\n",
    "\n",
    "        if fDrop != []:\n",
    "            data = data.drop(fDrop,\n",
    "                            axis=1)\n",
    "\n",
    "        # Create LGB mats\n",
    "        lData = lgb.Dataset(data, label=labels,\n",
    "                            free_raw_data=False,\n",
    "                            feature_name=list(data.columns),\n",
    "                            categorical_feature=['ID_code'])\n",
    "\n",
    "        return lData, labels, IDs, data\n",
    "\n",
    "\n",
    "# Specify columns to drop\n",
    "fDrop = []\n",
    "\n",
    "# Split training data in to training and validation sets.\n",
    "# Validation set is used for early stopping.\n",
    "trainData, validData = train_test_split(df_train,\n",
    "                                        test_size=0.3,\n",
    "                                        stratify=df_train.target)\n",
    "\n",
    "# Prepare the data sets\n",
    "trainDataL, trainLabels, trainIDs, trainData = prepLGB(trainData,\n",
    "                                                       classCol='target',\n",
    "                                                       IDCol='ID_code',\n",
    "                                                       fDrop=fDrop)\n",
    "\n",
    "validDataL, validLabels, validIDs, validData = prepLGB(validData,\n",
    "                                                       classCol='target',\n",
    "                                                       IDCol='ID_code',\n",
    "                                                       fDrop=fDrop)\n",
    "\n",
    "testDataL, _, _ , testData = prepLGB(df_test,\n",
    "                                     classCol='target',\n",
    "                                     IDCol='ID_code',\n",
    "                                     fDrop=fDrop)\n",
    "\n",
    "# Prepare data set using all the training data\n",
    "allTrainDataL, allTrainLabels, _ , allTrainData = prepLGB(df_train,\n",
    "                                                          classCol='target',\n",
    "                                                          IDCol='ID_code',\n",
    "                                                          fDrop=fDrop)\n",
    "\n",
    "gridParams = {'learning_rate': [0.005],\n",
    "              'n_estimators': [40],\n",
    "              'num_leaves': [6,8,12,16],\n",
    "              'boosting_type' : ['gbdt'],\n",
    "              'objective' : ['binary'],\n",
    "              'random_state' : [420], # Updated from 'seed'\n",
    "              'colsample_bytree' : [0.65, 0.66],\n",
    "              'subsample' : [0.7,0.75],\n",
    "              'reg_alpha' : [1,1.2],\n",
    "              'reg_lambda' : [1,1.2,1.4],}\n",
    "\n",
    "# Create parameters to search\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': 'binary',\n",
    "          'nthread': 3, # Updated from nthread\n",
    "          'num_leaves': 64,\n",
    "          'learning_rate': 0.05,\n",
    "          'max_bin': 512,\n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1,\n",
    "          'subsample_freq': 1,\n",
    "          'colsample_bytree': 0.8,\n",
    "          'reg_alpha': 5,\n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5,\n",
    "          'min_child_weight': 1,\n",
    "          'min_child_samples': 5,\n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'auc',\n",
    "          'n_jobs' : -1,\n",
    "          'device' : 'gpu'}\n",
    "\n",
    "mdl = lgb.LGBMClassifier(boosting_type= 'gbdt',\n",
    "          objective = 'binary',\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'],\n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'],\n",
    "          subsample_freq = params['subsample_freq'],\n",
    "          min_split_gain = params['min_split_gain'],\n",
    "          min_child_weight = params['min_child_weight'],\n",
    "          min_child_samples = params['min_child_samples'],\n",
    "          scale_pos_weight = params['scale_pos_weight'])\n",
    "\n",
    "# To view the default model params:\n",
    "mdl.get_params().keys()\n",
    "\n",
    "# Create the grid\n",
    "grid = GridSearchCV(mdl, gridParams,\n",
    "                    verbose=10,\n",
    "                    cv=4,\n",
    "                    n_jobs=6)\n",
    "# Run the grid\n",
    "grid.fit(allTrainData, allTrainLabels)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "\n",
    "# Using parameters already set above, replace in the best from the grid search\n",
    "params['colsample_bytree'] = grid.best_params_['colsample_bytree']\n",
    "params['learning_rate'] = grid.best_params_['learning_rate']\n",
    "# params['max_bin'] = grid.best_params_['max_bin']\n",
    "params['num_leaves'] = grid.best_params_['num_leaves']\n",
    "params['reg_alpha'] = grid.best_params_['reg_alpha']\n",
    "params['reg_lambda'] = grid.best_params_['reg_lambda']\n",
    "params['subsample'] = grid.best_params_['subsample']\n",
    "# params['subsample_for_bin'] = grid.best_params_['subsample_for_bin']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Fitting with params: ')\n",
    "print(params)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "label_cols = [\"target\"]\n",
    "y_split = df_train[label_cols].values\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(y_split[:,0], y_split[:,0])):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "\n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][ae_columns], label=df_train['target'].iloc[trn_idx])\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][ae_columns], label=df_train['target'].iloc[val_idx])\n",
    "\n",
    "    num_round = 100000\n",
    "    clf = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], valid_names=['train', 'test'],\n",
    "                    verbose_eval=100, early_stopping_rounds=200)\n",
    "    oof[val_idx] = clf.predict(df_train.iloc[val_idx][ae_columns], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = features\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(df_test[ae_columns], num_iteration=clf.best_iteration) / 5\n",
    "\n",
    "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\n",
    "sub_df['target'] = np.int32(predsTest > 0.5)\n",
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_original.to_csv(\"df_original.csv\", index=False)\n",
    "df_train.to_csv(\"auto_model_reconstructions/train_auto_0.csv\", index=False)\n",
    "df_test.to_csv(\"auto_model_reconstructions/test_auto_0.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
